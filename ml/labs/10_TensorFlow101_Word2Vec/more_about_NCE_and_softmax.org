Taken from this post:https://stats.stackexchange.com/a/245452/154812

* The issue

There are some issues with learning the word vectors using an "standard" neural
network. In this way, the word vectors are learned while the network learns to
predict the next word given a window of words (the input of the network).

Predicting the next word is like predicting the class. That is,

#+BEGIN_QUOTE
*such a network is just a "standard" multinomial (multi-class) classifier*.
#+END_QUOTE

And this network must have as many output neurons as classes there are. When
classes are actual words, the number of neurons is, well, huge.

A "standard" neural network is usually trained with a cross-entropy cost
function which requires the values of the output neurons to represent
probabilities - which means that the output "scores" computed by the network for
each class have to be normalized, converted into actual probabilities for each
class. This normalization step is achieved by means of the softmax function.

#+BEGIN_QUOTE
*Softmax is very costly when applied to a huge output layer*.
#+END_QUOTE

* The (a) solution

In order to deal with this issue, that is, the expensive computation of the
softmax, Word2Vec uses a technique called noise-contrastive estimation. This
technique was introduced by [A] (reformulated by [B]) then used in [C], [D], [E]
to learn word embeddings from unlabelled natural language text.

The basic idea is to

#+BEGIN_QUOTE
*convert a multinomial classification problem (as it is the problem of predicting
the next word) to a binary classification problem.*
#+END_QUOTE

That is, instead of using softmax to estimate a true probability distribution of
the output word, a *binary logistic regression* (binary classification) is used
instead.

For each training sample, the enhanced (optimized) classifier is fed a true pair
(a center word and another word that appears in its context) and a number of kk
randomly corrupted pairs (consisting of the center word and a randomly chosen
word from the vocabulary). By learning to

#+BEGIN_QUOTE
*distinguish the true pairs from corrupted ones, the classifier will ultimately
learn the word vectors.*
#+END_QUOTE

This is important:

#+BEGIN_QUOTE
*instead of predicting the next word (the "standard" training technique), the
optimized classifier simply predicts whether a pair of words is good or bad.*
#+END_QUOTE

Word2Vec slightly customizes the process and calls it *negative sampling*. In
Word2Vec, the words for the negative samples (used for the corrupted pairs) are
drawn from a specially designed distribution, which favours less frequent words
to be drawn more often.



References

[A] (2005) - Contrastive estimation: Training log-linear models on unlabeled data

[B] (2010) - Noise-contrastive estimation: A new estimation principle for unnormalized statistical models

[C] (2008) - A unified architecture for natural language processing: Deep neural networks with multitask learning

[D] (2012) - A fast and simple algorithm for training neural probabilistic language models.

[E] (2013) - Learning word embeddings efficiently with noise-contrastive estimation.
