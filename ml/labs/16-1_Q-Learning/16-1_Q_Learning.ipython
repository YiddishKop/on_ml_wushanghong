# import package needed
%matplotlib inline
import matplotlib.pyplot as plt
import os
os.environ["SDL_VIDEODRIVER"] = "dummy"  # this line make pop-out window not appear
import numpy as np
from ple.games.flappybird import FlappyBird
from ple import PLE

game = FlappyBird()
env = PLE(game, fps=30, display_screen=False)  # environment interface to game
env.reset_game()

couldn't import doomish
Couldn't import doom

# return a dictionary whose key is action description and value is action index
print(game.actions)
# return a list of action index (include None)
print(env.getActionSet())

{'up': 119}
[119, None]

game.getGameState()  # a dictionary describe state

{'next_next_pipe_bottom_y': 260,
 'next_next_pipe_dist_to_player': 427.0,
 'next_next_pipe_top_y': 160,
 'next_pipe_bottom_y': 244,
 'next_pipe_dist_to_player': 283,
 'next_pipe_top_y': 144,
 'player_vel': 0,
 'player_y': 256}

import math
import copy
from collections import defaultdict
MIN_EXPLORING_RATE = 0.01
MIN_LEARNING_RATE = 0.5

class Agent:

  def __init__(self,
               bucket_range_per_feature,
               num_action,
               t=0,
               discount_factor=0.99):
    self.update_parameters(t)  # init explore rate and learning rate
    self.q_table = defaultdict(lambda: np.zeros(num_action))
    self.discount_factor = discount_factor
    self.num_action = num_action

    # how to discretize each feature in a state
    # the higher each value, less time to train but with worser performance
    # e.g. if range = 2, feature with value 1 is equal to feature with value 0 bacause int(1/2) = int(0/2)
    self.bucket_range_per_feature = bucket_range_per_feature

  def select_action(self, state):
    # epsilon-greedy
    state_idx = self.get_state_idx(state)
    if np.random.rand() < self.exploring_rate:
      action = np.random.choice(num_action)  # Select a random action
    else:
      action = np.argmax(
          self.q_table[state_idx])  # Select the action with the highest q
    return action

  def update_policy(self, state, action, reward, state_prime):
    state_idx = self.get_state_idx(state)
    state_prime_idx = self.get_state_idx(state_prime)
    # Update Q_value using Q-learning update rule
    best_q = np.max(self.q_table[state_prime_idx])
    self.q_table[state_idx][action] += self.learning_rate * (
        reward + self.discount_factor * best_q - self.q_table[state_idx][action])

  def get_state_idx(self, state):
    # instead of using absolute position of pipe, use relative position
    state = copy.deepcopy(state)
    state['next_next_pipe_bottom_y'] -= state['player_y']
    state['next_next_pipe_top_y'] -= state['player_y']
    state['next_pipe_bottom_y'] -= state['player_y']
    state['next_pipe_top_y'] -= state['player_y']

    # sort to make list converted from dict ordered in alphabet order
    state_key = [k for k, v in sorted(state.items())]

    # do bucketing to decrease state space to speed up training
    state_idx = []
    for key in state_key:
      state_idx.append(int(state[key] / self.bucket_range_per_feature[key]))
    return tuple(state_idx)

  def update_parameters(self, episode):
    self.exploring_rate = max(MIN_EXPLORING_RATE,
                              min(0.5, 0.99**((episode) / 30)))
    self.learning_rate = max(MIN_LEARNING_RATE, min(0.5, 0.99
                                                    **((episode) / 30)))

  def shutdown_explore(self):
    # make action selection greedy
    self.exploring_rate = 0

num_action = len(env.getActionSet())
bucket_range_per_feature = {
  'next_next_pipe_bottom_y': 40,
  'next_next_pipe_dist_to_player': 512,
  'next_next_pipe_top_y': 40,
  'next_pipe_bottom_y': 20,
  'next_pipe_dist_to_player': 20,
  'next_pipe_top_y': 20,
  'player_vel': 4,
  'player_y': 16
}
# init agent
agent = Agent(bucket_range_per_feature, num_action)

def make_anim(images, fps=60, true_image=False):
  duration = len(images) / fps
  import moviepy.editor as mpy

  def make_frame(t):
    try:
      x = images[int(len(images) / duration * t)]
    except:
      x = images[-1]

    if true_image:
      return x.astype(np.uint8)
    else:
      return ((x + 1) / 2 * 255).astype(np.uint8)

  clip = mpy.VideoClip(make_frame, duration=duration)
  clip.fps = fps
  return clip

from IPython.display import Image, display

reward_per_epoch = []
lifetime_per_epoch = []
exploring_rates = []
learning_rates = []
print_every_episode = 500
show_gif_every_episode = 5000
NUM_EPISODE = 50000
for episode in range(0, NUM_EPISODE):

  # Reset the environment
  env.reset_game()

  # record frame
  frames = [env.getScreenRGB()]

  # for every 500 episodes, shutdown exploration to see performance of greedy action
  if episode % print_every_episode == 0:
    agent.shutdown_explore()

  # the initial state
  state = game.getGameState()
  cum_reward = 0  # cumulate reward for this episode
  t = 0

  while not env.game_over():

    # select an action
    action = agent.select_action(state)

    # execute the action and get reward
    reward = env.act(
        env.getActionSet()[action])  # reward = +1 when pass a pipe, -5 when die

    frames.append(env.getScreenRGB())

    # cumulate reward
    cum_reward += reward

    # observe the result
    state_prime = game.getGameState()  # get next state

    # update agent
    agent.update_policy(state, action, reward, state_prime)

    # Setting up for the next iteration
    state = state_prime
    t += 1

  # update exploring_rate and learning_rate
  agent.update_parameters(episode)

  if episode % print_every_episode == 0:
    print("Episode %d finished after %f time steps" % (episode, t))
    print("cumulated reward: %f" % cum_reward)
    print("exploring rate %f" % agent.exploring_rate)
    print("learning rate %f" % agent.learning_rate)
    reward_per_epoch.append(cum_reward)
    exploring_rates.append(agent.exploring_rate)
    learning_rates.append(agent.learning_rate)
    lifetime_per_epoch.append(t)

  # for every 5000 episode, record an animation
  if episode % show_gif_every_episode == 0:
    print("len frames:", len(frames))
    clip = make_anim(frames, fps=60, true_image=True).rotate(-90)
    display(clip.ipython_display(fps=60, autoplay=1, loop=1))

Episode 0 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.500000
learning rate 0.500000
len frames: 63

98%|█████████▊| 63/64 [00:00<00:00, 82.42it/s]

Episode 500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.500000
learning rate 0.500000
Episode 1000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.500000
learning rate 0.500000
Episode 1500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.500000
learning rate 0.500000
Episode 2000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.500000
learning rate 0.500000
Episode 2500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.432779
learning rate 0.500000
Episode 3000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.366032
learning rate 0.500000
Episode 3500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.309580
learning rate 0.500000
Episode 4000 finished after 59.000000 time steps
cumulated reward: -5.000000
exploring rate 0.261834
learning rate 0.500000
Episode 4500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.221452
learning rate 0.500000

8%|▊         | 5/64 [00:00<00:01, 48.27it/s]

Episode 5000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.187298
learning rate 0.500000
len frames: 63

98%|█████████▊| 63/64 [00:00<00:00, 79.99it/s]

Episode 5500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.158411
learning rate 0.500000
Episode 6000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.133980
learning rate 0.500000
Episode 6500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.113316
learning rate 0.500000
Episode 7000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.095840
learning rate 0.500000
Episode 7500 finished after 61.000000 time steps
cumulated reward: -5.000000
exploring rate 0.081059
learning rate 0.500000
Episode 8000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.068557
learning rate 0.500000
Episode 8500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.057984
learning rate 0.500000
Episode 9000 finished after 59.000000 time steps
cumulated reward: -5.000000
exploring rate 0.049041
learning rate 0.500000
Episode 9500 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.041477
learning rate 0.500000

8%|▊         | 8/100 [00:00<00:01, 79.60it/s]

Episode 10000 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.035080
learning rate 0.500000
len frames: 99

99%|█████████▉| 99/100 [00:01<00:00, 77.23it/s]

Episode 10500 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.029670
learning rate 0.500000
Episode 11000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.025094
learning rate 0.500000
Episode 11500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.021224
learning rate 0.500000
Episode 12000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.017951
learning rate 0.500000
Episode 12500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.015182
learning rate 0.500000
Episode 13000 finished after 74.000000 time steps
cumulated reward: -4.000000
exploring rate 0.012841
learning rate 0.500000
Episode 13500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010860
learning rate 0.500000
Episode 14000 finished after 77.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 14500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000

12%|█▎        | 8/64 [00:00<00:00, 78.65it/s]

Episode 15000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
len frames: 63

98%|█████████▊| 63/64 [00:00<00:00, 86.60it/s]

Episode 15500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 16000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 16500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 17000 finished after 111.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 17500 finished after 130.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 18000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 18500 finished after 218.000000 time steps
cumulated reward: 0.000000
exploring rate 0.010000
learning rate 0.500000
Episode 19000 finished after 75.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 19500 finished after 69.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000

3%|▎         | 4/149 [00:00<00:03, 39.44it/s]

Episode 20000 finished after 147.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
len frames: 148

99%|█████████▉| 148/149 [00:01<00:00, 94.73it/s]

Episode 20500 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 21000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 21500 finished after 73.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 22000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 22500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 23000 finished after 147.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 23500 finished after 177.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 24000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 24500 finished after 114.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000

8%|▊         | 9/116 [00:00<00:01, 83.63it/s]

Episode 25000 finished after 114.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
len frames: 115

99%|█████████▉| 115/116 [00:01<00:00, 73.80it/s]

Episode 25500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 26000 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 26500 finished after 247.000000 time steps
cumulated reward: 0.000000
exploring rate 0.010000
learning rate 0.500000
Episode 27000 finished after 366.000000 time steps
cumulated reward: 4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 27500 finished after 175.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 28000 finished after 402.000000 time steps
cumulated reward: 4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 28500 finished after 179.000000 time steps
cumulated reward: -1.000000
exploring rate 0.010000
learning rate 0.500000
Episode 29000 finished after 78.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 29500 finished after 324.000000 time steps
cumulated reward: 2.000000
exploring rate 0.010000
learning rate 0.500000

2%|▏         | 3/136 [00:00<00:04, 28.47it/s]

Episode 30000 finished after 134.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
len frames: 135

99%|█████████▉| 135/136 [00:01<00:00, 71.30it/s]

Episode 30500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 31000 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 31500 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 32000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 32500 finished after 444.000000 time steps
cumulated reward: 6.000000
exploring rate 0.010000
learning rate 0.500000
Episode 33000 finished after 75.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 33500 finished after 374.000000 time steps
cumulated reward: 4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 34000 finished after 106.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 34500 finished after 175.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000

0%|          | 2/855 [00:00<00:45, 18.90it/s]

Episode 35000 finished after 853.000000 time steps
cumulated reward: 16.000000
exploring rate 0.010000
learning rate 0.500000
len frames: 854

100%|█████████▉| 854/855 [00:13<00:00, 62.69it/s]

Episode 35500 finished after 213.000000 time steps
cumulated reward: -1.000000
exploring rate 0.010000
learning rate 0.500000
Episode 36000 finished after 247.000000 time steps
cumulated reward: 0.000000
exploring rate 0.010000
learning rate 0.500000
Episode 36500 finished after 149.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 37000 finished after 134.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 37500 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
Episode 38000 finished after 223.000000 time steps
cumulated reward: 0.000000
exploring rate 0.010000
learning rate 0.500000
Episode 38500 finished after 583.000000 time steps
cumulated reward: 9.000000
exploring rate 0.010000
learning rate 0.500000
Episode 39000 finished after 135.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 39500 finished after 146.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000

19%|█▉        | 12/64 [00:00<00:00, 119.87it/s]

Episode 40000 finished after 62.000000 time steps
cumulated reward: -5.000000
exploring rate 0.010000
learning rate 0.500000
len frames: 63

98%|█████████▊| 63/64 [00:00<00:00, 184.94it/s]

Episode 40500 finished after 131.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 41000 finished after 134.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 41500 finished after 247.000000 time steps
cumulated reward: 0.000000
exploring rate 0.010000
learning rate 0.500000
Episode 42000 finished after 114.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 42500 finished after 226.000000 time steps
cumulated reward: 0.000000
exploring rate 0.010000
learning rate 0.500000
Episode 43000 finished after 281.000000 time steps
cumulated reward: 1.000000
exploring rate 0.010000
learning rate 0.500000
Episode 43500 finished after 137.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 44000 finished after 147.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 44500 finished after 114.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000

19%|█▉        | 15/79 [00:00<00:00, 148.13it/s]

Episode 45000 finished after 77.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
len frames: 78

99%|█████████▊| 78/79 [00:00<00:00, 200.74it/s]

Episode 45500 finished after 190.000000 time steps
cumulated reward: -1.000000
exploring rate 0.010000
learning rate 0.500000
Episode 46000 finished after 98.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 46500 finished after 150.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 47000 finished after 303.000000 time steps
cumulated reward: 2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 47500 finished after 189.000000 time steps
cumulated reward: -1.000000
exploring rate 0.010000
learning rate 0.500000
Episode 48000 finished after 134.000000 time steps
cumulated reward: -3.000000
exploring rate 0.010000
learning rate 0.500000
Episode 48500 finished after 73.000000 time steps
cumulated reward: -4.000000
exploring rate 0.010000
learning rate 0.500000
Episode 49000 finished after 166.000000 time steps
cumulated reward: -2.000000
exploring rate 0.010000
learning rate 0.500000
Episode 49500 finished after 627.000000 time steps
cumulated reward: 10.000000
exploring rate 0.010000
learning rate 0.500000

def demo():
  # Reset the environment
  env.reset_game()

  # record frame
  frames = [env.getScreenRGB()]

  # for every 500 episodes, shutdown exploration to see performance of greedy action
  agent.shutdown_explore()

  # the initial state
  state = game.getGameState()

  while not env.game_over():
    # select an action
    action = agent.select_action(state)

    # execute the action and get reward
    reward = env.act(env.getActionSet()[action])

    frames.append(env.getScreenRGB())

    # observe the result
    state_prime = game.getGameState()  # get next state

    # Setting up for the next iteration
    state = state_prime

  clip = make_anim(frames, fps=60, true_image=True).rotate(-90)
  display(clip.ipython_display(fps=60, autoplay=1, loop=1))
demo()

100%|█████████▉| 528/529 [00:05<00:00, 94.41it/s]

# plot life time against training episodes
fig, ax1 = plt.subplots(figsize=(20, 5))
plt.plot(range(len(lifetime_per_epoch)), lifetime_per_epoch)
fig.tight_layout()
plt.show()

# plot reward against training episodes
import matplotlib.pyplot as plt
fig, ax1 = plt.subplots(figsize=(20, 5))
plt.plot(range(len(reward_per_epoch)), reward_per_epoch)
plt.show()
