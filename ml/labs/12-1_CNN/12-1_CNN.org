* Nuts and Bolts of Convolution Neural Networks

In this lab, we introduce two datasets, *mnist* and *cifar*, then we will talk
about how to implement CNN models for these two datasets using tensorflow. The
major difference between mnist and cifar is their size. Due to the limit of
memory size and time issue, we offer a guide to illustrate typical *input
pipeline* of tensorflow. Let's dive into tensorflow!

** MNIST Dataset[[MNIST-Dataset][¶]]

We start from a simple dataset. MNIST is a simple computer vision dataset. It
consists of images of handwritten digits like:

[[file:imgsrc/MNIST.png]]

It also includes labels for each image, telling us which digit it is. For
example, the labels for the above images are 5, 0, 4, and 1. Each image is 28
pixels by 28 pixels. We can interpret this as a big array of numbers:

[[file:imgsrc/MNIST2.png]]

The MNIST data is hosted on [[http://yann.lecun.com/exdb/mnist/][Yann LeCun's website]]. We can directly import MNIST
dataset from Tensorflow.


#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    from tensorflow.examples.tutorials.mnist import input_data
    import tensorflow as tf
    import os

    dest_directory = 'dataset/mnist'
    # check the directory
    if not os.path.exists(dest_directory):
      os.makedirs(dest_directory)
    # import data
    mnist = input_data.read_data_sets("dataset/mnist/", one_hot=True)
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    Extracting dataset/mnist/train-images-idx3-ubyte.gz
    Extracting dataset/mnist/train-labels-idx1-ubyte.gz
    Extracting dataset/mnist/t10k-images-idx3-ubyte.gz
    Extracting dataset/mnist/t10k-labels-idx1-ubyte.gz
#+END_SRC

*** Softmax Regression on MNIST[[Softmax-Regression-on-MNIST][¶]]
    :PROPERTIES:
    :CUSTOM_ID: Softmax-Regression-on-MNIST
    :END:

Before jumping to /Convolutional Neural Network/ model, we're going to start
with a very simple model with a single layer and softmax regression.

We know that every image in MNIST is a handwritten digit between zero and nine.
So there are only ten possible digits that a given image can be. We want to give
the probability of the input image for being each digit. That is, input an
image, the model outputs a ten-dimension vector.

This is a classic case where a softmax regression is a natural, simple model. If
you want to assign probabilities to an object being one of several different
things, softmax is the thing to do.

In [2]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Create the model (Softmax Regression)

    x = tf.placeholder(tf.float32,[None, 784])  # flatten into vector of 28 x 28 = 784
    y_true = tf.placeholder(tf.float32, [None, 10])  # true answers
    W = tf.Variable(tf.zeros([784, 10]))  # Weights
    b = tf.Variable(tf.zeros([10]))  # bias
    y_pred = tf.matmul(x, W) + b  # y = Wx + b

    # Define loss and optimizer
    cross_entropy = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=y_true, 
                                                logits=y_pred))  # our loss
    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(
        cross_entropy)  # our optimizer
#+END_SRC

After creating our model and defining the loss and optimizer, we can
start training.

In [3]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Training and Testing

    with tf.Session() as sess:
      # initialize the variables we created
      sess.run(tf.global_variables_initializer())  
      # run the training step 1000 times
      for _ in range(1000):
        batch_xs, batch_ys = mnist.train.next_batch(100)
        # feed training data x and y_ for training
        sess.run(train_step, feed_dict={
                x: batch_xs,
                y_true: batch_ys
            })  

      # Testing
      correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))
      accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
      accuracy = sess.run(accuracy_op, feed_dict={
              x: mnist.test.images,
              y_true: mnist.test.labels
          })
      # feed our testing data for testing
      print('Accuracy: %.1f%%' % (accuracy * 100))  
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    Accuracy: 91.8%
#+END_SRC

From the above result, we got about 92% accuracy for /Softmax
Regression/ on MNIST. In fact, it's not so good. This is because we're
using a very simple model.

*** Multilayer Convolutional Network on
MNIST[[Multilayer-Convolutional-Network-on-MNIST][¶]]
    :PROPERTIES:
    :CUSTOM_ID: Multilayer-Convolutional-Network-on-MNIST
    :END:

We're now jumping from a very simple model to something moderately
sophisticated: a small /Convolutional Neural Network/. This will get us to
around 99.2% accuracy, not state of the art, but respectable.

Here is the diagram of the model we're going to build:

[[file:imgsrc/mnist_deep.png]]

To create this model, we need to create a lot of weights and biases. Instead of
doing this repeatedly, let's create two handy functions to do it for us.

In [4]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    def weight_variable(shape):
      initial = tf.truncated_normal(shape, stddev=0.1)
      return tf.Variable(initial)

    def bias_variable(shape):
      initial = tf.constant(0.1, shape=shape)
      return tf.Variable(initial)
#+END_SRC

TensorFlow gives us a lot of flexibility in *convolution* and *pooling*
operations. How do we handle the boundaries? What is our stride size? For now,
we're going to choose the vanilla version. To keep our code cleaner, let's also
abstract those operations into functions.

In [5]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input.
    # Our pooling is plain old max pooling over 2x2 blocks.

    def conv2d(x, W):
      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

    def max_pool_2x2(x):
      return tf.nn.max_pool(
          x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
#+END_SRC

We can now implement our layers.

In [6]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # [batch_size, height, width, channel]
    x_image = tf.reshape(x, [-1, 28, 28, 1])

    # First Convolutional Layer
    W_conv1 = weight_variable([5, 5, 1, 32]) # (filter_height, filter_width, number of input channels, number of output channels)
    b_conv1 = bias_variable([32])

    # convolve x_image with the weight tensor, add the bias, then apply the ReLU function
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    # and finally max pool 
    h_pool1 = max_pool_2x2(h_conv1) # It will reduce the image size to "14x14"
#+END_SRC

In [7]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Second Convolutional Layer

    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])

    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool_2x2(h_conv2) # It will reduce the image size to "7x7"
#+END_SRC

Now that the image size has been reduced to 7x7, we add a fully-connected layer
with 1024 neurons to allow processing on the entire image.

In [8]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Densely Connected Layer

    W_fc1 = weight_variable([7 * 7 * 64, 1024]) 
    b_fc1 = bias_variable([1024])

    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) # flatten
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
#+END_SRC

To reduce overfitting, we will apply [[https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf][dropout]] before the readout layer. The idea
behind dropout is to train an ensemble of model instead of a single model.
During training, we drop out neurons with probability $p$, i.e., the probability
to keep is $1-p$. When a neuron is dropped, its output is set to zero. These
dropped neurons do not contribute to the training phase in forward pass and
backward pass. For each training phase, we train the network slightly different
from the previous one. It's just like we train different networks in each
training phrase. However, during testing phase, we *don't* drop any neuron, and
thus, implement dropout is kind of like doing ensemble. Also, randomly drop
units in training phase can prevent units from co-adapting too much. Thus,
dropout is a powerful regularization techique to deal with /overfitting/.

We create a placeholder for the probability that a neuron's output is kept
during dropout. This allows us to turn dropout on during training, and turn it
off during testing.

In [9]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Dropout

    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
#+END_SRC

Finally, we add a layer, just like for the one layer softmax regression
above.

In [10]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Readout Layer

    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])

    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
#+END_SRC

After defining our model, we then define our loss and optimizer.

In [11]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Define loss and optimizer

    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_conv)) # our loss
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # our optimizer
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_true, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
#+END_SRC

Let's check how well does this model do! Note that we will include the
additional parameter *keep\_prob* in feed\_dict to control the dropout
rate.

In [16]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Training and Testing

    # Re-import data for initializing batch
    mnist = input_data.read_data_sets("dataset/mnist", one_hot=True)

    with tf.Session() as sess:
      sess.run(
          tf.global_variables_initializer())  # initialize the variables we created
      # run the training step 20000 times
      for i in range(20000):
        batch = mnist.train.next_batch(50)
        if i % 1000 == 0:
          train_accuracy = accuracy.eval(feed_dict={
              x: batch[0],
              y_true: batch[1],
              keep_prob: 1.0
          })
          print('step %d, training accuracy %.1f%%' % (i, train_accuracy * 100))
        train_step.run(feed_dict={
            x: batch[0],
            y_true: batch[1],
            keep_prob: 0.5
        })  # feed into x, y_ and keep_prob for training

      print('test accuracy %.1f%%' % (100 * accuracy.eval(feed_dict={
          x: mnist.test.images,
          y_true: mnist.test.labels,
          keep_prob: 1.0
      })))  # feed for testing
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    Extracting dataset/mnist/train-images-idx3-ubyte.gz
    Extracting dataset/mnist/train-labels-idx1-ubyte.gz
    Extracting dataset/mnist/t10k-images-idx3-ubyte.gz
    Extracting dataset/mnist/t10k-labels-idx1-ubyte.gz
    step 0, training accuracy 14.0%
    step 1000, training accuracy 98.0%
    step 2000, training accuracy 96.0%
    step 3000, training accuracy 100.0%
    step 4000, training accuracy 98.0%
    step 5000, training accuracy 100.0%
    step 6000, training accuracy 100.0%
    step 7000, training accuracy 100.0%
    step 8000, training accuracy 100.0%
    step 9000, training accuracy 100.0%
    step 10000, training accuracy 100.0%
    step 11000, training accuracy 100.0%
    step 12000, training accuracy 100.0%
    step 13000, training accuracy 100.0%
    step 14000, training accuracy 100.0%
    step 15000, training accuracy 100.0%
    step 16000, training accuracy 100.0%
    step 17000, training accuracy 100.0%
    step 18000, training accuracy 98.0%
    step 19000, training accuracy 100.0%
    test accuracy 99.2%
#+END_SRC

The final testing accuracy should be approximately 99.2%

** Cifar-10[[Cifar-10][¶]]

Actually MNIST is a easy dataset for the beginner. To demonstrate the power of
/Neural Networks/, we need a larger dataset /CIFAR-10/.

[[https://www.cs.toronto.edu/~kriz/cifar.html][CIFAR-10]] consists of 60000 32x32 color images in 10 classes, with 6000 images
per class. There are 50000 training images and 10000 test images. Here are the
classes in the dataset, as well as 10 random images from each:

[[file:imgsrc/CIFAR10.png]]

Before jumping to a complicated neural network model, we're going to start with
*KNN* and *SVM*. The motivation here is to compare neural network model with
traditional classifiers, and highlight the performance of neural network model.

*** K Nearest Neighbors (KNN) on CIFAR-10

Keras offers convenient facilities that automatically download some well-known
datasets and store them in the ~/.keras/datasets directory. Let's load the
CIFAR-10 in Keras:

In [17]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # Loading Data
    from keras.datasets import cifar10
    from keras.utils import np_utils
    import numpy as np
    import math

    (X_train, y_train), (X_test, y_test) = cifar10.load_data()
    # convert class vectors to binary vectors
    Y_train = np_utils.to_categorical(y_train)
    Y_test = np_utils.to_categorical(y_test)

    print('X_train shape:', X_train.shape)
    print('Y_train shape:', Y_train.shape)
    print('X_test shape:', X_test.shape)
    print('Y_test shape:', Y_test.shape)
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    X_train shape: (50000, 32, 32, 3)
    Y_train shape: (50000, 10)
    X_test shape: (10000, 32, 32, 3)
    Y_test shape: (10000, 10)
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    Using TensorFlow backend.
#+END_SRC

The datas are loaded as integers, so we need to cast it to floating point values
in order to perform the division:

In [18]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
  # Data Preprocessing
  # normalize inputs from 0-255 to 0.0-1.0
  X_train = X_train.astype('float32')
  X_test = X_test.astype('float32')
  X_train = X_train / 255.0
  X_test = X_test / 255.0
#+END_SRC

For simplicity, we also convert the images into the grayscale. We use the [[https://en.wikipedia.org/wiki/Grayscale#Luma_coding_in_video_systems][Luma
coding]] that is common in video systems:

In [20]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    import matplotlib.pyplot as plt
    # transform an 3-channel image into one channel
    def grayscale(data, dtype='float32'):
      # luma coding weighted average in video systems
      r = np.asarray(.3, dtype=dtype)
      g = np.asarray(.59, dtype=dtype)
      b = np.asarray(.11, dtype=dtype)
      rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]
      # add channel dimension
      rst = np.expand_dims(rst, axis=3)
      return rst

    X_train_gray = grayscale(X_train)
    X_test_gray = grayscale(X_test)

    # plot a randomly chosen image
    img = round(np.random.rand() * X_train.shape[0])
    plt.figure(figsize=(4, 2))
    plt.subplot(1, 2, 1)
    plt.imshow(X_train[img], interpolation='none')
    plt.subplot(1, 2, 2)
    plt.imshow(
        X_train_gray[img, :, :, 0], cmap=plt.get_cmap('gray'), interpolation='none')
    plt.show()
#+END_SRC

As we can see, the objects in grayscale images can still be recognizable.

**** Feature Selection
     :PROPERTIES:
     :CUSTOM_ID: Feature-Selection
     :END:

When coming to object detection, HOG (histogram of oriented gradients) is often
extracted as a feature for classification. It first calculates the gradients of
each image patch using sobel filter, then use the magnitudes and orientations of
derived gradients to form a histogram per patch (a vector). After normalizing
these histograms, it concatenates them into one HOG feature. For more details,
read this [[https://www.learnopencv.com/histogram-of-oriented-gradients/][tutorial]].

#+BEGIN_QUOTE
  Note. one can directly feed the original images for classification;
  however, it will take lots of time to train and get worse performance.
#+END_QUOTE

In [21]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # The code is credit to: "http://www.itdadao.com/articles/c15a1243072p0.html"
    def getHOGfeat(image,
                   stride=8,
                   orientations=8,
                   pixels_per_cell=(8, 8),
                   cells_per_block=(2, 2)):
      cx, cy = pixels_per_cell
      bx, by = cells_per_block
      sx, sy, sz = image.shape
      n_cellsx = int(np.floor(sx // cx))  # number of cells in x
      n_cellsy = int(np.floor(sy // cy))  # number of cells in y
      n_blocksx = (n_cellsx - bx) + 1
      n_blocksy = (n_cellsy - by) + 1
      gx = np.zeros((sx, sy), dtype=np.double)
      gy = np.zeros((sx, sy), dtype=np.double)
      eps = 1e-5
      grad = np.zeros((sx, sy, 2), dtype=np.double)
      for i in range(1, sx - 1):
        for j in range(1, sy - 1):
          gx[i, j] = image[i, j - 1] - image[i, j + 1]
          gy[i, j] = image[i + 1, j] - image[i - 1, j]
          grad[i, j, 0] = np.arctan(gy[i, j] / (gx[i, j] + eps)) * 180 / math.pi
          if gx[i, j] < 0:
            grad[i, j, 0] += 180
          grad[i, j, 0] = (grad[i, j, 0] + 360) % 360
          grad[i, j, 1] = np.sqrt(gy[i, j]**2 + gx[i, j]**2)
      normalised_blocks = np.zeros((n_blocksy, n_blocksx, by * bx * orientations))
      for y in range(n_blocksy):
        for x in range(n_blocksx):
          block = grad[y * stride:y * stride + 16, x * stride:x * stride + 16]
          hist_block = np.zeros(32, dtype=np.double)
          eps = 1e-5
          for k in range(by):
            for m in range(bx):
              cell = block[k * 8:(k + 1) * 8, m * 8:(m + 1) * 8]
              hist_cell = np.zeros(8, dtype=np.double)
              for i in range(cy):
                for j in range(cx):
                  n = int(cell[i, j, 0] / 45)
                  hist_cell[n] += cell[i, j, 1]
              hist_block[(k * bx + m) * orientations:(k * bx + m + 1) * orientations] = hist_cell[:]
          normalised_blocks[y, x, :] = hist_block / np.sqrt(
              hist_block.sum()**2 + eps)
      return normalised_blocks.ravel()
#+END_SRC

Once we have our /getHOGfeat/ function, we then get the HOG features of all
images.

In [22]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    X_train_hog = []
    X_test_hog = []

    print('This will take some minutes.')

    for img in X_train_gray:
      img_hog = getHOGfeat(img)
      X_train_hog.append(img_hog)

    for img in X_test_gray:
      img_hog = getHOGfeat(img)
      X_test_hog.append(img_hog)

    X_train_hog_array = np.asarray(X_train_hog)
    X_test_hog_array = np.asarray(X_test_hog)
#+END_SRC

[[http://scikit-learn.org/stable/supervised_learning.html#supervised-learning][scikit-learn]] provides off-the-shelf libraries for classification. For KNN and
SVM classifiers, we can just import from scikit-learn to use.

In [23]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # KNN
    from sklearn.neighbors import KNeighborsClassifier 
    from sklearn.metrics import accuracy_score

    # p=2 and metric='minkowski' means the Euclidean Distance
    knn = KNeighborsClassifier(n_neighbors=11, p=2, metric='minkowski')

    knn.fit(X_train_hog_array, y_train.ravel())
    y_pred = knn.predict(X_test_hog_array)
    print('[KNN]')
    print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())
    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    [KNN]
    Misclassified samples: 5334
    Accuracy: 0.47
#+END_SRC

We can observe that the accuracy of KNN on CIFAR-10 is embarrassingly
bad.

*** Support Vector Machine (SVM) on CIFAR-10

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # SVM
    from sklearn.svm import SVC 

    # C is the hyperparameter for the error penalty term
    # gamma is the hyperparameter for the rbf kernel
    svm_linear = SVC(kernel='linear', random_state=0, gamma=0.2, C=10.0)

    svm_linear.fit(X_train_hog_array, y_train.ravel())
    y_pred = svm_linear.predict(X_test_hog_array)
    print('[Linear SVC]')
    print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())
    print('Accuracy: %.2f' % accuracy_score(y_test.ravel(), y_pred))
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    [Linear SVC]
    Misclassified samples: 4940
    Accuracy: 0.51
#+END_SRC

By above, SVM is slightly better than KNN, but still poor. Next, we'll
design a CNN model using tensorflow. Because the cifar10 is not a small
dataset, we can't just use feed\_dict to feed all training data to the
model due to the limit of memory size. Even if we can feed all data into
the model, we still want the process of loading data is efficient.
*Input pipeline* is the common way to solve these.

** Input Pipeline[[Input-Pipeline][¶]]

*** Queues[[Queues][¶]]

Because ~tf.Session~ objects are designed to be *multithreaded* and thread-safe,
so multiple threads can easily use the same session and run ops in parallel.
[[https://www.tensorflow.org/programmers_guide/threading_and_queues][Queues]] are useful because of the ability to *compute tensor asynchronously* in a
graph. Most of the time, we use queues to handle inputs. In this way, multiple
threads prepare training example and enequeue these examples. In addition, only
parts of inputs would be read into memory a time, instead of all of them. This
can avoid *out of memory error* when data is large.

#+BEGIN_QUOTE
  Tensorflow recommended queue-base input pipeline before version 1.2. Beginning
  with version 1.2, tensorflow recommend using the [[https://www.tensorflow.org/programmers_guide/datasets][tf.contrib.data module]]
  instead. Read [[https://github.com/tensorflow/tensorflow/issues/7951][more]].
#+END_QUOTE

*** Typical Input Pipeline[[Typical-Input-Pipeline][¶]]

1. The list of filenames
2. Optional filename shuffling
3. Optional epoch limit
4. Filename queue
5. A Reader for the file format
6. A decoder for a record read by the reader
7. Optional preprocessing
8. Example queue

[[file:imgsrc/AnimatedFileQueues.gif]] We've specified the order of

input pipeline in the followng codes.

In [1]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    import os
    import sys
    from six.moves import urllib
    import tarfile
    import tensorflow as tf
    import numpy as np
#+END_SRC

*** Loading Data Manually[[Loading-Data-Manually][¶]]

To know how it works under the hood, let's load CIFAR-10 by our own (not using
keras). According the descripion, the dataset file is divided into five training
batches and one test batch, each with 10000 images. The test batch contains
exactly 1000 randomly-selected images from each class. We define some constants
based on the above:

In [2]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # the url to download CIFAR-10 dataset (binary version)
    # see format and details here: http://www.cs.toronto.edu/~kriz/cifar.html
    DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'
    DEST_DIRECTORY = 'dataset/cifar10'
    # the image size we want to keep
    IMAGE_HEIGHT = 32
    IMAGE_WIDTH = 32
    IMAGE_DEPTH = 3
    IMAGE_SIZE_CROPPED = 24
    BATCH_SIZE = 128
    # Global constants describing the CIFAR-10 data set.
    NUM_CLASSES = 10 
    NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000
    NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000
#+END_SRC

In [3]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    def maybe_download_and_extract(dest_directory, url):
      if not os.path.exists(dest_directory):
        os.makedirs(dest_directory)
      file_name = 'cifar-10-binary.tar.gz'
      file_path = os.path.join(dest_directory, file_name)
      # if have not downloaded yet
      if not os.path.exists(file_path):
        def _progress(count, block_size, total_size):
          sys.stdout.write('\r%.1f%%' % 
                (float(count * block_size) / float(total_size) * 100.0))
          sys.stdout.flush()  # flush the buffer

        print('>> Downloading %s ...' % file_name)
        file_path, _ = urllib.request.urlretrieve(url, file_path, _progress)
        file_size = os.stat(file_path).st_size
        print('\r>> Total %d bytes' % file_size)
      extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')
      if not os.path.exists(extracted_dir_path):
        # Open for reading with gzip compression, then extract all
        tarfile.open(file_path, 'r:gz').extractall(dest_directory)
      print('>> Done')

    # download it
    maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    >> Downloading cifar-10-binary.tar.gz ...
    >> Total 170052171 bytes
    >> Done
#+END_SRC

After downloading the dataset, we create functions

-  =distort_input(training_file, batch_size)= to get a training example
   queue.
-  =eval_input(testing_file, batch_size)= to get a testing example
   queue.
-  =read_cifar10(filename_queue)= to read a record from dataset with a
   filename queue.

In [4]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # the folder store the dataset
    DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'
    # (1) a list of training/testing filenames
    training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1,6)]
    testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]
#+END_SRC

In [5]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # (5) + (6)
    def read_cifar10(filename_queue):
      """ Reads and parses examples from CIFAR10 data files.
        -----
        Args:
            filename_queue: 
                A queue of strings with the filenames to read from.
        Returns:
            An object representing a single example, with the following fields:
            height: 
                number of rows in the result (32)
            width: 
                number of columns in the result (32)
            depth: 
                number of color channels in the result (3)
            key: 
                a scalar string Tensor describing the filename & record number for this example.
            label: 
                an int32 Tensor with the label in the range 0..9.
            image: 
                a [height, width, depth] uint8 Tensor with the image data
      """

      class CIFAR10Record(object):
        pass

      result = CIFAR10Record()
      # CIFAR10 consists of 60000 32x32 'color' images in 10 classes
      label_bytes = 1  # 10 class
      result.height = IMAGE_HEIGHT
      result.width = IMAGE_WIDTH
      result.depth = IMAGE_DEPTH
      image_bytes = result.height * result.width * result.depth
      # bytes of a record: label(1 byte) followed by pixels(3072 bytes)
      record_bytes = label_bytes + image_bytes
      # (5) reader for cifar10 file format
      reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)
      # read a record
      result.key, record_string = reader.read(filename_queue)
      # Convert from a string to a vector of uint8 that is record_bytes long.
      # (6) decoder
      record_uint8 = tf.decode_raw(record_string, tf.uint8)
      # get the label and cast it to int32
      result.label = tf.cast(
          tf.strided_slice(record_uint8, [0], [label_bytes]), tf.int32)
      # [depth, height, width], uint8
      depth_major = tf.reshape(
          tf.strided_slice(record_uint8, [label_bytes],
                           [label_bytes + image_bytes]),
          [result.depth, result.height, result.width])
      # change to [height, width, depth], uint8
      result.image = tf.transpose(depth_major, [1, 2, 0])
      return result
#+END_SRC

In [6]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    def distort_input(training_files, batch_size):
      """ Construct distorted input for CIFAR training using the Reader ops.
        -----
        Args:
            training_files: 
                an array of paths of the training files.
            batch_size: 
                Number of images per batch.
        Returns:
            images: Images. 
                4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.
            labels: Labels. 
                1D tensor of [batch_size] size.
      """
      for f in training_files:
        if not tf.gfile.Exists(f):
          raise ValueError('Failed to find file: ' + f)
      # create a queue that produces filenames to read
      # (4) filename queue
      file_queue = tf.train.string_input_producer(training_files)
      # (5) + (6)
      cifar10_record = read_cifar10(file_queue)
      # (7) image preprocessing for training
      height = IMAGE_SIZE_CROPPED
      width = IMAGE_SIZE_CROPPED
      float_image = tf.cast(cifar10_record.image, tf.float32)
      distorted_image = tf.random_crop(float_image, [height, width, 3])
      distorted_image = tf.image.random_flip_left_right(distorted_image)
      distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)
      distorted_image = tf.image.random_contrast(
          distorted_image, lower=0.2, upper=1.8)
      # standardization: subtract off the mean and divide by the variance of the pixels
      distorted_image = tf.image.per_image_standardization(distorted_image)
      # Set the shapes of tensors.
      distorted_image.set_shape([height, width, 3])
      cifar10_record.label.set_shape([1])
      # ensure a level of mixing of elements.
      min_fraction_of_examples_in_queue = 0.4
      min_queue_examples = int(
          NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * min_fraction_of_examples_in_queue)
      # (8) example queue
      # Filling queue with min_queue_examples CIFAR images before starting to train
      image_batch, label_batch = tf.train.shuffle_batch(
          [distorted_image, cifar10_record.label],
          batch_size=batch_size,
          num_threads=16,
          capacity=min_queue_examples + 3 * batch_size,
          min_after_dequeue=min_queue_examples)
      return image_batch, tf.reshape(label_batch, [batch_size])
#+END_SRC

The following code is to generate the data for testing. Now, you are able to
specify the order of input pipeline in the following code block.

In [7]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    def eval_input(testing_files, batch_size):
      for f in testing_files:
        if not tf.gfile.Exists(f):
          raise ValueError('Failed to find file: ' + f)
      # create a queue that produces filenames to read
      file_queue = tf.train.string_input_producer(testing_files)
      cifar10_record = read_cifar10(file_queue)
      # image preprocessing for training
      height = IMAGE_SIZE_CROPPED
      width = IMAGE_SIZE_CROPPED
      float_image = tf.cast(cifar10_record.image, tf.float32)
      resized_image = tf.image.resize_image_with_crop_or_pad(
          float_image, height, width)
      image_eval = tf.image.per_image_standardization(resized_image)
      image_eval.set_shape([height, width, 3])
      cifar10_record.label.set_shape([1])
      # Ensure that the random shuffling has good mixing properties.
      min_fraction_of_examples_in_queue = 0.4
      min_queue_examples = int(
          NUM_EXAMPLES_PER_EPOCH_FOR_EVAL * min_fraction_of_examples_in_queue)
      image_batch, label_batch = tf.train.batch(
          [image_eval, cifar10_record.label],
          batch_size=batch_size,
          num_threads=16,
          capacity=min_queue_examples + 3 * batch_size)
      return image_batch, tf.reshape(label_batch, [batch_size])
#+END_SRC

After building the input pipeline, we can check the functionality of the example
queues.

In [8]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # test function distort_input
    with tf.Session() as sess:
      coord = tf.train.Coordinator()
      image, label = distort_input(training_files, BATCH_SIZE)
      # --- Note ---
      # If you forget to call start_queue_runners(), it will hang
      # indefinitely and deadlock the user program.
      # ------------
      threads = tf.train.start_queue_runners(sess=sess, coord=coord)
      image_batch, label_batch = sess.run([image, label])
      coord.request_stop()
      coord.join(threads)
      image_batch_np = np.asarray(image_batch)
      label_batch_np = np.asarray(label_batch)
      print('Shape of cropped image:', image.shape)
      print('Shape of label:', label.shape)
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    Shape of cropped image: (128, 24, 24, 3)
    Shape of label: (128,)
#+END_SRC

So far, we have prepared input queues. Let's start designing our cnn model!

** CNN Model
*** Model Structure

[[file:imgsrc/model.png]]

*** Model Details

-  We put all variables on CPU because we want GPU to only focus on
   calculation.
-  The cost function we use is simply the /cross entropy/ of labels and
   predictions.
-  /Weight decay/ is a very common regularization technique. For NNs, we
   can penalize large weights in the cost function. The implementation
   of weight decay is simple: add a term in the cost function that
   penalizes the $L\^{2}$-norm of the weight matrix at each layer.
   $$\operatorname{arg}\underset{\Theta=\{\boldsymbol{W\^{(1)}}{\cdots}\boldsymbol{W\^{(L)}}\}}{\operatorname{min}}C(\Theta)+\alpha\sum\_{i=1}\^{L}
   \lVert \boldsymbol{W\^{(i)}} \rVert\_{2}\^{2}$$
-  /Local response normalization/ is mentioned in original
   [[http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf][/AlexNet/]]
   article in NIPS 2012. Because the activation function we used in our
   CNN model is /ReLU/, whose output has no upper bound. Thus, we need a
   local response normalization to normalize that.\\
   Denoting by $a\_{x,y}\^i$ the activity of a neuron computed by
   applying kernel $i$ at position $(x, y)$ and then applying the ReLU
   nonlinearity, the response-normalized activity $b\^i\_{x,y}$ is given
   by the expression $$ b\^i\_{x,y} = a\^i\_{x,y} / \left( k + \alpha
   \sum\_{j=max(0,i-n/2)}\^{min(N-1, i+n/2)} (a\^j\_{x,y})\^2
   \right)\^\beta$$ where the sum runs over $n$ *adjacent* kernel maps
   at the same spatial position, and $N$ is the total number of kernels
   in the layer. The ordering of the kernel maps is arbitrary and
   determined before training begins. The constants $k$, $n$, $\alpha$,
   and $\beta$ are hyper-parameters. Check the following figure drawn by
   Hu Yixuan.

    [[file:imgsrc/localResponseNormalization.jpeg]]

-  When using gradient descent to update the weights of a neural network,
  sometimes the weights might move in the wrong direction. Thus, we take a
  [[https://www.tensorflow.org/versions/r0.12/api_docs/python/train/moving_averages][moving average]] of the weights over a bunch of previous updates.

   $$\boldsymbol{w\_{avg\_i}} = decay\times\boldsymbol{w\_{avg\_{i-1}}}
   + (1-decay)\times\boldsymbol{w\_{i}}$$ where $w\_{i}$ is the
   $i\_{th}$ updated weight.

In [9]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    class CNN_Model(object):
      def __init__(self, batch_size, num_classes, num_training_example,
                   num_epoch_per_decay, init_lr, moving_average_decay):
        self.batch_size = batch_size
        self.num_classes = num_classes
        self.num_training_example = num_training_example
        self.num_epoch_per_decay = num_epoch_per_decay
        self.init_lr = init_lr  # initial learn rate
        self.moving_average_decay = moving_average_decay

      def _variable_on_cpu(self, name, shape, initializer):
        with tf.device('/cpu:0'):
          var = tf.get_variable(
              name, shape, initializer=initializer, dtype=tf.float32)
        return var

      def _variable_with_weight_decay(self, name, shape, stddev, wd=0.0):
        """ Helper to create an initialized Variable with weight decay.
            Note that the Variable is initialized with a truncated normal 
            distribution. A weight decay is added only if one is specified.
            -----
            Args:
                name: 
                    name of the variable
                shape: 
                    a list of ints
                stddev: 
                    standard deviation of a truncated Gaussian
                wd: 
                    add L2Loss weight decay multiplied by this float. If None, weight
                    decay is not added for this Variable.
            Returns:
                Variable Tensor
        """
        initializer = tf.truncated_normal_initializer(
            stddev=stddev, dtype=tf.float32)
        var = self._variable_on_cpu(name, shape, initializer)
        # deal with weight decay
        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')
        tf.add_to_collection('losses', weight_decay)
        return var

      def inference(self, images):
        """ build the model
            -----
            Args:
                images with shape [batch_size,24,24,3]
            Return:
                logits with shape [batch_size,10]
        """
        with tf.variable_scope('conv_1') as scope:
          kernel = self._variable_with_weight_decay('weights', [5, 5, 3, 64], 5e-2)
          conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding="SAME")
          biases = self._variable_on_cpu('bias', [64], tf.constant_initializer(0.0))
          pre_activation = tf.nn.bias_add(conv, biases)
          conv_1 = tf.nn.relu(pre_activation, name=scope.name)
        # pool_1
        pool_1 = tf.nn.max_pool(
            conv_1,
            ksize=[1, 3, 3, 1],
            strides=[1, 2, 2, 1],
            padding='SAME',
            name='pool_1')
        # norm_1 (local_response_normalization)
        norm_1 = tf.nn.lrn(
            pool_1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_1')
        # conv2
        with tf.variable_scope('conv_2') as scope:
          kernel = self._variable_with_weight_decay('weights', [5, 5, 64, 64], 5e-2)
          conv = tf.nn.conv2d(norm_1, kernel, [1, 1, 1, 1], padding='SAME')
          biases = self._variable_on_cpu('biases', [64],
                                         tf.constant_initializer(0.1))
          pre_activation = tf.nn.bias_add(conv, biases)
          conv_2 = tf.nn.relu(pre_activation, name=scope.name)
        # norm2
        norm_2 = tf.nn.lrn(
            conv_2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_2')
        # pool2
        pool_2 = tf.nn.max_pool(
            norm_2,
            ksize=[1, 3, 3, 1],
            strides=[1, 2, 2, 1],
            padding='SAME',
            name='pool_2')
        # FC_1 (fully-connected layer)
        with tf.variable_scope('FC_1') as scope:
          flat_features = tf.reshape(pool_2, [self.batch_size, -1])
          dim = flat_features.get_shape()[1].value
          weights = self._variable_with_weight_decay('weights', [dim, 384], 0.04,
                                                     0.004)
          biases = self._variable_on_cpu('biases', [384],
                                         tf.constant_initializer(0.1))
          FC_1 = tf.nn.relu(
              tf.matmul(flat_features, weights) + biases, name=scope.name)
        # FC_2
        with tf.variable_scope('FC_2') as scope:
          weights = self._variable_with_weight_decay('weights', [384, 192], 0.04,
                                                     0.004)
          biases = self._variable_on_cpu('biases', [192],
                                         tf.constant_initializer(0.1))
          FC_2 = tf.nn.relu(tf.matmul(FC_1, weights) + biases, name=scope.name)
        with tf.variable_scope('softmax_linear') as scope:
          weights = self._variable_with_weight_decay(
              'weights', [192, self.num_classes], 1 / 192.0)
          biases = self._variable_on_cpu('biases', [self.num_classes],
                                         tf.constant_initializer(0.0))
          logits = tf.add(tf.matmul(FC_2, weights), biases, name=scope.name)
        return logits

      def loss(self, logits, labels):
        '''calculate the loss'''
        labels = tf.cast(labels, tf.int64)
        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
            labels=labels, logits=logits, name='cross_entropy_per_example')
        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')
        tf.add_to_collection('losses', cross_entropy_mean)
        # The total loss is defined as the cross entropy loss plus all of the weight
        # decay terms (L2 loss).
        return tf.add_n(tf.get_collection('losses'), name='total_loss')

      def train(self, total_loss, global_step):
        '''train a step'''
        num_batches_per_epoch = self.num_training_example / self.batch_size
        decay_steps = int(num_batches_per_epoch * self.num_epoch_per_decay)
        # Decay the learning rate exponentially based on the number of steps.
        lr = tf.train.exponential_decay(
            self.init_lr, global_step, decay_steps, decay_rate=0.1, staircase=True)
        opt = tf.train.GradientDescentOptimizer(lr)
        grads = opt.compute_gradients(total_loss)
        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)
        # Track the moving averages of all trainable variables.
        # This step just records the moving average weights but not uses them
        ema = tf.train.ExponentialMovingAverage(self.moving_average_decay,
                                                global_step)
        self.ema = ema
        variables_averages_op = ema.apply(tf.trainable_variables())
        with tf.control_dependencies([apply_gradient_op, variables_averages_op]):
          train_op = tf.no_op(name='train')
        return train_op
#+END_SRC

Now, we can train our model. First, we need to feed some hyperparameters
into it.

In [10]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    tf.reset_default_graph()
    # CNN model
    model = CNN_Model(batch_size=BATCH_SIZE, 
                      num_classes=NUM_CLASSES, 
                      num_training_example=NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN, 
                      num_epoch_per_decay=350.0, 
                      init_lr=0.1,
                      moving_average_decay=0.9999)
#+END_SRC

Here we use CPU to handle the input because we want GPU to only focus on
training.

In [11]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    # op for training
    global_step = tf.contrib.framework.get_or_create_global_step()
    with tf.device('/cpu:0'):
      images, labels = distort_input(training_files, BATCH_SIZE)
    with tf.variable_scope('model'):
      logits = model.inference(images)
    total_loss = model.loss(logits, labels)
    train_op = model.train(total_loss, global_step)
#+END_SRC

Next, we train our model 180 epochs and save it.

In [12]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    NUM_EPOCH = 180
    NUM_BATCH_PER_EPOCH = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN // BATCH_SIZE
    ckpt_dir = './model/'

    # train
    saver = tf.train.Saver()
    with tf.Session() as sess:
      ckpt = tf.train.get_checkpoint_state(ckpt_dir)
      if (ckpt and ckpt.model_checkpoint_path):
        saver.restore(sess, ckpt.model_checkpoint_path)
        # assume the name of checkpoint is like '.../model.ckpt-1000'
        gs = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])
        sess.run(tf.assign(global_step, gs))
      else:
        # no checkpoint found
        sess.run(tf.global_variables_initializer())
      coord = tf.train.Coordinator()
      threads = tf.train.start_queue_runners(sess=sess, coord=coord)
      loss = []
      for i in range(NUM_EPOCH):
        _loss = []
        for _ in range(NUM_BATCH_PER_EPOCH):
          l, _ = sess.run([total_loss, train_op])
          _loss.append(l)
        loss_this_epoch = np.sum(_loss)
        gs = global_step.eval()
        # print('loss of epoch %d: %f' % (gs / NUM_BATCH_PER_EPOCH, loss_this_epoch))
        loss.append(loss_this_epoch)
        saver.save(sess, ckpt_dir + 'model.ckpt', global_step=gs)
      coord.request_stop()
      coord.join(threads)
      
    print('Done')
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    Done
#+END_SRC

We have done our training! Let's see whether our model is great or not.

In [13]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    with tf.device('/cpu:0'):
        # build testing example queue
        images, labels = eval_input(testing_files, BATCH_SIZE)
    with tf.variable_scope('model', reuse=True):
        logits = model.inference(images)
    # use to calculate top-1 error
    top_k_op = tf.nn.in_top_k(logits, labels, 1) 
#+END_SRC

Because now the weights are not moving average weights, we need to
manually change this.

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    tf.train.ExponentialMovingAverage(decay).variables_to_restore()
#+END_SRC

gives us a dictionary about the mapping between the weights and the
moving average shadow weights. We can use this mapping to replace the
original weights by moving average shadow weights.

In [14]:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    variables_to_restore = model.ema.variables_to_restore()
    saver = tf.train.Saver(variables_to_restore)
    with tf.Session() as sess:
      # Restore variables from disk.
      ckpt = tf.train.get_checkpoint_state(ckpt_dir)
      if ckpt and ckpt.model_checkpoint_path:
        saver.restore(sess, ckpt.model_checkpoint_path)
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        num_iter = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL // BATCH_SIZE
        total_sample_count = num_iter * BATCH_SIZE
        true_count = 0
        for _ in range(num_iter):
          predictions = sess.run(top_k_op)
          true_count += np.sum(predictions)
        print('Accurarcy: %d/%d = %f' % (true_count, total_sample_count,
                                         true_count / total_sample_count))
        coord.request_stop()
        coord.join(threads)
      else:
        print('train first')
#+END_SRC

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    INFO:tensorflow:Restoring parameters from ./model/model.ckpt-70200
    Accurarcy: 8584/9984 = 0.859776
#+END_SRC

We get a much higher accuracy than KNN and SVM. This is good enough!

* Assignment

Implement the input pipeline of the CNN model with [[https://www.tensorflow.org/programmers_guide/datasets][dataset]] API mentioned last
lab. The dataset should be multithreaded (16 threads). To simplify, you only
need to train the model for 10 epochs. Finally, get the accuracy of this
10-epoch model. There are 4 'TODO' parts you need to finish. You only need to
hand out the Lab12\_{id}.ipynb.\\

The notebook should include

-  Training loss per epoch
-  The testing accuracy
-  The total time to train and test

Good luck!


#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    from lab12_util import *

    DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'
    DEST_DIRECTORY = 'dataset/cifar10'
    DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'
    IMAGE_HEIGHT = 32
    IMAGE_WIDTH = 32
    IMAGE_DEPTH = 3
    IMAGE_SIZE_CROPPED = 24
    BATCH_SIZE = 128
    NUM_CLASSES = 10 
    LABEL_BYTES = 1
    IMAGE_BYTES = 32 * 32 * 3
    NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000
    NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000

    # download it
    maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)
#+END_SRC


#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    from tensorflow.contrib.data import FixedLengthRecordDataset, Iterator

    def cifar10_record_distort_parser(record):
      ''' Parse the record into label, cropped and distorted image
        -----
        Args:
            record: 
                a record containing label and image.
        Returns:
            label: 
                the label in the record.
            image: 
                the cropped and distorted image in the record.
      '''
      # TODO1
      pass

    def cifar10_record_crop_parser(record):
      ''' Parse the record into label, cropped image
        -----
        Args:
            record: 
                a record containing label and image.
        Returns:
            label: 
                the label in the record.
            image: 
                the cropped image in the record.
      '''
      # TODO2
      pass

    def cifar10_iterator(filenames, batch_size, cifar10_record_parser):
      ''' Create a dataset and return a tf.contrib.data.Iterator 
        which provides a way to extract elements from this dataset.
        -----
        Args:
            filenames: 
                a tensor of filenames.
            batch_size: 
                batch size.
        Returns:
            iterator: 
                an Iterator providing a way to extract elements from the created dataset.
            output_types: 
                the output types of the created dataset.
            output_shapes: 
                the output shapes of the created dataset.
      '''
      record_bytes = LABEL_BYTES + IMAGE_BYTES
      dataset = FixedLengthRecordDataset(filenames, record_bytes)
      # TODO3
      # tips: use dataset.map with cifar10_record_parser(record)
      #       output_types = dataset.output_types
      #       output_shapes = dataset.output_shapes
      pass
#+END_SRC


#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    tf.reset_default_graph()

    training_files = [
        os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1, 6)]
    testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]

    filenames_train = tf.constant(training_files)
    filenames_test = tf.constant(testing_files)

    iterator_train, types, shapes = cifar10_iterator(filenames_train, BATCH_SIZE,
                                                     cifar10_record_distort_parser)
    iterator_test, _, _ = cifar10_iterator(filenames_test, BATCH_SIZE,
                                           cifar10_record_crop_parser)

    # use to handle training and testing
    handle = tf.placeholder(tf.string, shape=[])
    iterator = Iterator.from_string_handle(handle, types, shapes)
    labels_images_pairs = iterator.get_next()

    # CNN model
    model = CNN_Model(
        batch_size=BATCH_SIZE,
        num_classes=NUM_CLASSES,
        num_training_example=NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN,
        num_epoch_per_decay=350.0,
        init_lr=0.1,
        moving_average_decay=0.9999)

    with tf.device('/cpu:0'):
      labels, images = labels_images_pairs
      labels = tf.reshape(labels, [BATCH_SIZE])
      images = tf.reshape(
          images, [BATCH_SIZE, IMAGE_SIZE_CROPPED, IMAGE_SIZE_CROPPED, IMAGE_DEPTH])
    with tf.variable_scope('model'):
      logits = model.inference(images)
    # train
    global_step = tf.contrib.framework.get_or_create_global_step()
    total_loss = model.loss(logits, labels)
    train_op = model.train(total_loss, global_step)
    # test
    top_k_op = tf.nn.in_top_k(logits, labels, 1)
#+END_SRC


#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
    %%time

    # TODO4:
    # 1. train the CNN model 10 epochs
    # 2. show the loss per epoch
    # 3. get the accuracy of this 10-epoch model
    # 4. measure the time using '%%time' instruction
    # tips:
    # use placeholder handle to determine if training or testing. 
#+END_SRC

