{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DataLab Cup 1: Predicting Appropriate Response<center>\n",
    "<center>Shan-Hung Wu &amp; DataLab<br>Fall 2017</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this competition, you have to **select the most appropriate response from 6 candidates** based on previous chat message. You are provided with lines of total 8 tv programs as training data, and each program has serveral episodes. You also get a question collection which contains **1 chat history and 6 condidate responses** for each question. Your goal is to learn a function that is able to predict the best response.\n",
    "<img src='illustration.png' width='25%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Format\n",
    "+ ```Program01.csv~Program08.csv``` contains total 8 tv program's lines\n",
    "+ ```Question.csv``` contains total 500 questions, and each question includes chat and candidate options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Submit Results? \n",
    "You have to predict the correct response in ```Question.csv```, and **submit it to the Kaggle-In-Class** online judge system. Following are some example actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Action | Description |\n",
    "| - | - |\n",
    "| Data | Get the dataset. |\n",
    "| Make a Submission | Your testing performance will be evaluated immediately and shown on the leaderboard. |\n",
    "| Leaderboard | The current ranking of participants. Note that this ranking only reflects the performance on part of the testset and may not equal to the final ranking (see below). |\n",
    "| Forum | You can ask questions or share findings here. |\n",
    "| Kernels | You can create your jupyter notebook, run it, and keep it as private or public here. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring\n",
    "The evaluation metric is **CategorizationAccuracy**. The ranking shown on the leaderboard before the end of competition reflects only the accuracy over **part of** ```Question.csv```. However, this is **not** how we evaluate your final scores. After the competition, we combine **accuracy over the entire ```Question.csv```** and **your report** as the final score.\n",
    "<br>\n",
    "<br>\n",
    "There will be two baseline results, namely, ```Benchmark-60``` and ```Benchmark-80```. You have to outperform ```Benchmark-60``` to get 60 points, and ```Benchmark-80``` to get 80. Meanwhile, **the higher accuracy you achieve, the higher the final score you will get**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Dates\n",
    "+ 2017/10/24 (TUE) - competition starts\n",
    "+ 2017/10/29 (SUN) 23:59pm - competition ends, final score announcement\n",
    "+ 2017/10/31 (TUE) - winner team share\n",
    "+ 2017/11/2 (THU) 23:59pm - report submission (iLMS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "After the competition, **each team have to hand in a report** in Jupyter notebook format via the iLMS system. You report should include:\n",
    "+ Student ID, name of each team member\n",
    "+ How did you preprocess data (cleaning, feature engineering, etc.)?\n",
    "+ How did you build the classifier (model, training algorithm, special techniques, etc.)?\n",
    "+ Conclusions (interesting findings, pitfalls, takeaway lessons, etc.)?\n",
    "\n",
    "The file name of your report must be ```DL_comp1_{Your Team number}_report.ipynb```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hint 1: Feature Engineering is More Important Then You Expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we learn various machine learning techniques based on datasets where the date features are predefined. In many real-world applications, including this competition, we only get raw data and have to define the features ourself. **Feature engineering** is the process of using domain knowledge to create features that make machine learning algorithms work. While good modeling and training techniques help you make better predictions, feature engineering usually determines whether your task is \"learnable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program 1\n",
      "Episodes: 1299\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    還好天氣不錯\\n昨天晚上的流星雨\\n我看到很多流星\\n這次的收穫真豐富\\n當然豐富啦\\n我就...\n",
      "1    好熱喔\\n這種倉庫很不通風\\n好熱喔\\n受不了\\n今天天氣真的是太熱了\\n我都快中暑了\\n那...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 2\n",
      "Episodes: 205\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    我們現在只差兩分\\n只差兩分\\n等下阿偉先站過來\\n他們會埋伏一個射手出來\\n我們盡量把他堵...\n",
      "1    四十年前\\n我媽為了養我跟我哥\\n開這間理髮店\\n她把手藝都傳給哥\\n希望他可以接下這間店\\...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 3\n",
      "Episodes: 57\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    台南人劇團\\n一個從古都台南輸出的\\n現代劇團\\n每年總有令人驚奇的戲劇產生\\n玩大師\\n莎...\n",
      "1    一齣舞台劇\\n這個舞台劇的結果\\n所有觀眾都知道\\n兩個演員在舞台上\\n撐了一百多分鐘\\n目...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 4\n",
      "Episodes: 10\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    念書幹嘛偷光\\n燈一開就有了啊\\n太陽是不是從樹葉之間的\\n這個縫灑下來\\n然後在地上啊\\n...\n",
      "1    倫語社\\n效果立竿見影\\n立竿見影\\n這也是指很快囉\\n但是它和曇花一現\\n有什麼不一樣\\n...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 5\n",
      "Episodes: 369\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    公平的對待\\n孩子才會樂於做良性的競爭\\n這樣一來\\n真正有實力的人才不會被埋沒\\n老師好\\...\n",
      "1    你們看\\n我臉上的痘痘\\n畫了粧之後就沒那麼明顯了\\n就算熬夜K書\\n長了黑眼圈也不怕\\n你...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 6\n",
      "Episodes: 80\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    在這個世上\\n既能解放你滿肚子壓力\\n又讓你避之唯恐不及的\\n只有馬桶\\n但是如果你到現在還...\n",
      "1    你相信嗎\\n全球十大致人於死的動物榜首是誰\\n獅子嗎\\n不是\\n鱷魚嗎\\nNo\\n答案竟然是...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 7\n",
      "Episodes: 611\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    嗨, 大家好\\n歡迎收看「聽聽看」\\n你這個禮拜過得好不好呢\\n有沒有什麼新鮮事\\n要和朋友...\n",
      "1    你今天是不是跟我一樣\\n早就迫不及待的\\n想要收看我們「聽聽看」了呢\\n怎麼樣\\n上個禮拜的...\n",
      "Name: Content, dtype: object\n",
      "\n",
      "Program 8\n",
      "Episodes: 210\n",
      "Index(['Content'], dtype='object')\n",
      "\n",
      "0    每天帶你拜訪一個家庭\\n邀請一位貴賓和他們共進晚餐\\n談談人生大小事\\n但如果登門拜訪的\\n...\n",
      "1    如果用一句話\\n來形容吃飯這件事情\\n那句話應該就是體驗人生\\n今天的「誰來晚餐」\\n發生在...\n",
      "Name: Content, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NUM_PROGRAM = 8\n",
    "programs = []\n",
    "for i in range(1, NUM_PROGRAM+1):\n",
    "    program = pd.read_csv('Program0%d.csv' % (i))\n",
    "    \n",
    "    print('Program %d' % (i))\n",
    "    print('Episodes: %d' % (len(program)))\n",
    "    print(program.columns)\n",
    "    print()\n",
    "    \n",
    "    print(program.loc[:1]['Content'])\n",
    "    print()\n",
    "    \n",
    "    programs.append(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "Episodes: 500\n",
      "Index(['Question', 'Option0', 'Option1', 'Option2', 'Option3', 'Option4',\n",
      "       'Option5'],\n",
      "      dtype='object')\n",
      "\n",
      "0    媽給你送錢包來啦 來 你看一下是不是這個\\n對 就是這個 你在哪裡找到它的\\n\n",
      "1             古人說三日不讀書 面目可憎 我覺得我最近可能臉色太難看了\\n\n",
      "2                         你說我們做父母的最擔心的就是這個\\n\n",
      "Name: Question, dtype: object\n",
      "\n",
      "0        你看 這是我新買的錢包\n",
      "1    所以想回復我昔日面貌姣好的樣子\n",
      "2      我剛剛聽你媽說你要讀什麼科\n",
      "Name: Option0, dtype: object\n",
      "\n",
      "0     我的錢包不見了啦\n",
      "1    是不是要定期來舉辦\n",
      "2    其他老師又集體叛變\n",
      "Name: Option1, dtype: object\n",
      "\n",
      "0    以後上網咖的錢包在我身上\n",
      "1         各辦理一次才對\n",
      "2        聽起來好好玩天啊\n",
      "Name: Option2, dtype: object\n",
      "\n",
      "0                          什麼有錢包場\n",
      "1                     能夠督促所有的用人機關\n",
      "2    只是小孩自己的興趣不能得到發展 他們的心裡可能也會很悶喔\n",
      "Name: Option3, dtype: object\n",
      "\n",
      "0    早上你爸爸在車上找到的 一定是前天你放學的時候掉在車上了\n",
      "1                   在上次的節目討論中也有提到\n",
      "2                      走到這裡就沒有路了耶\n",
      "Name: Option4, dtype: object\n",
      "\n",
      "0         我為什麼要給你們錢包\n",
      "1           超過九十分貝以上\n",
      "2    每一個科目像是國語數學都很優秀\n",
      "Name: Option5, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = pd.read_csv('Question.csv')\n",
    "\n",
    "print('Question')\n",
    "print('Episodes: %d' % (len(questions)))\n",
    "print(questions.columns)\n",
    "print()\n",
    "\n",
    "print(questions.loc[:2]['Question'])\n",
    "print()\n",
    "\n",
    "for i in range(6):\n",
    "    print(questions.loc[:2]['Option%d' % (i)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get raw content of programs' lines, but there aren't any feature we can learn from. To predict from text, we have to go through several preprocessing steps first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Cut Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since chinese characters are continuous one by one, we have to **cut them into meaningful words** first. We use [jieba](https://github.com/fxsjy/jieba) with **traditional chinese dictionary** to cut our text. You can install **jieba** via pip.\n",
    "```\n",
    "    pip install jieba\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('big5_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/tim/ray/Workspace/Course_DeepLearning/Comp1/Release/big5_dict.txt ...\n",
      "Loading model from cache /tmp/jieba.ubfd2136d7a9b93dc278d35ab3e6630e5.cache\n",
      "Loading model cost 0.544 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '討厭', '吃', '蘋果']\n"
     ]
    }
   ],
   "source": [
    "example_str = '我討厭吃蘋果'\n",
    "cut_example_str = jieba.lcut(example_str)\n",
    "print(cut_example_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We cut not only ```Program.csv``` but also ```Question.csv```, and save as **list**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jieba_lines(lines):\n",
    "    cut_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        cut_line = jieba.lcut(line)\n",
    "        cut_lines.append(cut_line)\n",
    "    \n",
    "    return cut_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_programs = []\n",
    "\n",
    "for program in programs:\n",
    "    n = len(program)\n",
    "    cut_program = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        lines = program.loc[i]['Content'].split('\\n')\n",
    "        cut_program.append(jieba_lines(lines))\n",
    "    \n",
    "    cut_programs.append(cut_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1299\n",
      "635\n",
      "[['還好', '天氣', '不錯'], ['昨天', '晚上', '的', '流星雨'], ['我', '看到', '很多', '流星']]\n"
     ]
    }
   ],
   "source": [
    "print(len(cut_programs))\n",
    "print(len(cut_programs[0]))\n",
    "print(len(cut_programs[0][0]))\n",
    "print(cut_programs[0][0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_questions = []\n",
    "n = len(questions)\n",
    "\n",
    "for i in range(n):\n",
    "    cut_question = []\n",
    "    lines = questions.loc[i]['Question'].split('\\n')\n",
    "    cut_question.append(jieba_lines(lines))\n",
    "    \n",
    "    for j in range(6):\n",
    "        line = questions.loc[j]['Option%d' % (j)]\n",
    "        cut_question.append(jieba.lcut(line))\n",
    "    \n",
    "    cut_questions.append(cut_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "7\n",
      "[['媽給', '你', '送', '錢包', '來', '啦', ' ', '來', ' ', '你', '看', '一下', '是', '不', '是', '這個'], ['對', ' ', '就是', '這個', ' ', '你', '在', '哪裡', '找到', '它', '的'], []]\n",
      "['你', '看', ' ', '這是', '我', '新', '買', '的', '錢包']\n",
      "['是', '不', '是', '要', '定期', '來', '舉辦']\n",
      "['聽起來', '好好玩', '天', '啊']\n",
      "['那', '我', '去', '探索', '一下']\n",
      "['什麼', '你', '說', '我', '是', '鬼']\n",
      "['沒有', '人', '是', '十全十美', '的']\n"
     ]
    }
   ],
   "source": [
    "print(len(cut_questions))\n",
    "print(len(cut_questions[0]))\n",
    "print(cut_questions[0][0])\n",
    "\n",
    "for i in range(1, 7):\n",
    "    print(cut_questions[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('cut_Programs.npy', cut_programs)\n",
    "np.save('cut_Questions.npy', cut_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving, we can load them directly next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_Question = np.load('cut_Questions.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Word Dictionary & Out-of-Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many words after cutting, but **not all of them is useful**. The word **too common or too rare** can not give us information but may noise. We count the the number of occurrence for each word and remove useless one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_word_dict(w):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for program in cut_programs:\n",
    "    for lines in program:\n",
    "        for line in lines:\n",
    "            for w in line:\n",
    "                add_word_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in cut_questions:\n",
    "    lines = question[0]\n",
    "    for line in lines:\n",
    "        for w in line:\n",
    "            add_word_dict(w)\n",
    "    \n",
    "    for i in range(1, 7):\n",
    "        line = question[i]\n",
    "        for w in line:\n",
    "            add_word_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('他', 81495), ('也', 77074), ('就是', 75444), ('說', 74677), ('來', 69134), ('會', 67805), ('那', 67274), ('喔', 61443), ('可以', 60159), ('跟', 59954)]\n"
     ]
    }
   ],
   "source": [
    "VOC_SIZE = 15000\n",
    "VOC_START = 20\n",
    "\n",
    "voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "print(voc_dict[:10])\n",
    "np.save('voc_dict.npy', voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_dict = np.load('voc_dict.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ```voc_dict``` becomes better word dictionary, then we should replace those removed words aka out-of-vocabulary words into an **unknown token** in the following use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Generating Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the format of question is to select one from six, our traing data only have continuous lines. Naively, i want to change the whole problem **into a binary classification** which means given two lines, my model want to **judge these two are context or not**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_TRAIN = 10000\n",
    "TRAIN_VALID_RATE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "            line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "            \n",
    "            Xs.append([cut_programs[program_id][episode_id][line_id], \n",
    "                       cut_programs[program_id][episode_id][line_id+1]])\n",
    "            Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            first_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            first_episode_id = random.randint(0, len(cut_programs[first_program_id])-1)\n",
    "            first_line_id = random.randint(0, len(cut_programs[first_program_id][first_episode_id])-1)\n",
    "            \n",
    "            second_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            second_episode_id = random.randint(0, len(cut_programs[second_program_id])-1)\n",
    "            second_line_id = random.randint(0, len(cut_programs[second_program_id][second_episode_id])-1)\n",
    "            \n",
    "            Xs.append([cut_programs[first_program_id][first_episode_id][first_line_id], \n",
    "                       cut_programs[second_program_id][second_episode_id][second_line_id]])\n",
    "            Ys.append(0)\n",
    "    \n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "x_train, y_train = Xs[:int(NUM_TRAIN*TRAIN_VALID_RATE)], Ys[:int(NUM_TRAIN*TRAIN_VALID_RATE)]\n",
    "x_valid, y_valid = Xs[int(NUM_TRAIN*TRAIN_VALID_RATE):], Ys[int(NUM_TRAIN*TRAIN_VALID_RATE):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since machine learning models only accept numerical features, we must convert categorical features, such as tokens into a numerical form. In the next section, we introduce several commonly used models, including **BoW**, **TF-IDF**, and **Feature Hashing** that allows us to represent text as numerical feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['還好 天氣 不錯 ', '昨天 晚上 ', '看到 很多 流星 ', '這次 收穫 真 豐富 ', '當然 豐富 啦 ', '說 嘛 ', '精心 製作 ', '被 一個 人 吃掉 ', '真的 嗎 ', '不要 忘記 做 秘密 檔案 ']\n"
     ]
    }
   ],
   "source": [
    "example_doc = []\n",
    "\n",
    "for line in cut_programs[0][0]:\n",
    "    example_line = ''\n",
    "    for w in line:\n",
    "        if w in voc_dict:\n",
    "            example_line += w+' '\n",
    "        \n",
    "    example_doc.append(example_line)\n",
    "\n",
    "print(example_doc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: BoW (Bag-Of-Words)\n",
    "The idea behind bag-of-words model is to represent each document by occurrence of words, which can be summarized as the following steps:\n",
    "1. Build vocabulary dictionary by unique token from the entire set of documents;\n",
    "2. Represent each document by a vector, where each position corresponds to the occurrence of a vocabulary in dictionary.\n",
    "\n",
    "Each vocabulary in BoW can be a single word (1-gram) or a sequence of n continuous words (n-gram). It has been shown empirically that 3-gram or 4-gram BoW models yield good performance in anti-spam email filtering application.\n",
    "<br>\n",
    "<br>\n",
    "Here, we use Scikit-learn's implementation [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to construct the BoW model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabulary]\n",
      "\n",
      "一半 7\n",
      "經常 377\n",
      "兩種 89\n",
      "跑進去 445\n",
      "地盤 156\n",
      "趕走 443\n",
      "常常 206\n",
      "脫皮 395\n",
      "更新 271\n",
      "現在 323\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ngram_range=(min, max), default: 1-gram => (1, 1)\n",
    "count = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "count.fit(example_doc)\n",
    "BoW = count.vocabulary_\n",
    "print('[vocabulary]\\n')\n",
    "for key in list(BoW.keys())[:10]:\n",
    "    print('%s %d' % (key, BoW[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter ```ngram_range=(min-length, max-length)``` in CountVectorizer specifies the vocabulary to be ```{min-length}```-gram to ```{max-length}```-gram. For example ```ngram_range=(1, 2)``` will use both 1-gram and 2-gram as vocabularies. After constructing BoW model by calling ```fit()```, you can access BoW vocabularies in its attribute ```vocubalary_```, which is stored as Python dictionary that maps vocabulary to an integer index.\n",
    "<br>\n",
    "<br>\n",
    "Let's transform the example documents into feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(did, vid)\ttf\n",
      "  (0, 46)\t1\n",
      "  (0, 168)\t1\n",
      "  (0, 469)\t1\n",
      "  (1, 268)\t1\n",
      "  (1, 270)\t1\n",
      "  (2, 217)\t1\n",
      "  (2, 310)\t1\n",
      "  (2, 352)\t1\n",
      "  (3, 259)\t1\n",
      "  (3, 435)\t1\n",
      "  (3, 456)\t1\n",
      "  (4, 340)\t1\n",
      "  (4, 435)\t1\n",
      "  (6, 370)\t1\n",
      "  (6, 414)\t1\n",
      "  (7, 6)\t1\n",
      "  (7, 128)\t1\n",
      "  (8, 354)\t1\n",
      "  (9, 44)\t1\n",
      "  (9, 225)\t1\n",
      "  (9, 293)\t1\n",
      "  (9, 361)\t1\n",
      "\n",
      "Is document-term matrix a scipy.sparse matrix? True\n"
     ]
    }
   ],
   "source": [
    "# get matrix (doc_id, vocabulary_id) --> tf\n",
    "doc_bag = count.transform(example_doc)\n",
    "print('(did, vid)\\ttf')\n",
    "print(doc_bag[:10])\n",
    "\n",
    "print('\\nIs document-term matrix a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each document contains only a small subset of vocabularies, ```CountVectorizer.transform()``` stores feature vectors as scipy.sparse matrix, where entry index is ```(document-index, vocabulary-index)``` pair, and the value is the **term frequency**---the number of times a vocabulary (term) occurs in a document.\n",
    "<br>\n",
    "<br>\n",
    "Unfortunately, many Scikit-learn classifiers do not support input as sparse matrix now. We can convert ```doc_bag``` into a Numpy dense matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "\n",
      "After calling .toarray(), is it a scipy.sparse matrix? False\n"
     ]
    }
   ],
   "source": [
    "doc_bag = doc_bag.toarray()\n",
    "print(doc_bag[:10])\n",
    "\n",
    "print('\\nAfter calling .toarray(), is it a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "蟋蟀: 98\n",
      "可以: 21\n",
      "就是: 21\n",
      "聲音: 20\n",
      "這樣: 19\n",
      "你們: 17\n",
      "真的: 16\n",
      "還有: 15\n",
      "比較: 15\n",
      "豆油伯: 15\n"
     ]
    }
   ],
   "source": [
    "doc_bag = count.fit_transform(example_doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones(bag_cnts.shape[0]))[0][bag_cnts.argsort()[::-1][:top]], \n",
    "                  np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('%s: %d' % (tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out most frequent words among documents, we first sum up vocabulary counts in documents, where ```axis=0``` is the document index. Then, we sort the summed vocabulary count array in ascending order and get the sorted index by ```argsort()```. Next, we revert the sorted list by ```[::-1]```, and feed into ```inverse_transform()``` to get corresponding vocabularies. Finally, we show the 10 most frequent vocabularies with their occurrence counts.\n",
    "<br>\n",
    "<br>\n",
    "Next, we introduce the **TF-IDF** model that **downweights frequently occurring words** among the input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: TF-IDF (Term-Frequency & Inverse-Document-Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-IDF model calculates not only the term-frequency (TF) as BoW model does, but also the **document-frequency**\n",
    "(DF) of a term, which refers to the number of documents that contain this term. The TF-IDF score for a term is defined as\n",
    "<img src='tf-idf.png' width='30%' />\n",
    "where the ```log()``` term is called the **inverse-document-frequency** (IDF) and ```Ndoc``` is the total number of documents. The idea behind TF-IDF is to downweight the TF of a word if it appears in many documents. For example, if a word appears in every document, the second term become ```log(1)+1=1``` , which will be smaller than any other word appearing in only a part of documents.\n",
    "<br>\n",
    "<br>\n",
    "NOTE: we add ```1``` to both the numerator and denominator inside the ```log()``` in the above definition so to avoid the numeric issue of dividing by ```0```.\n",
    "<br>\n",
    "<br>\n",
    "Let's create the TF-IDF feature representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "蟋蟀: 2.87\n",
      "可以: 4.36\n",
      "就是: 4.41\n",
      "聲音: 4.46\n",
      "這樣: 4.46\n",
      "你們: 4.56\n",
      "真的: 4.62\n",
      "還有: 4.68\n",
      "豆油伯: 4.68\n",
      "比較: 4.68\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "蟋蟀: 42.016104\n",
      "這樣: 11.916386\n",
      "真的: 11.405347\n",
      "就是: 11.256123\n",
      "可以: 10.898674\n",
      "聲音: 10.442999\n",
      "豆油伯: 10.325579\n",
      "還有: 9.835135\n",
      "你們: 9.293539\n",
      "叫做: 8.395597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf.fit(example_doc)\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' % (tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(example_doc).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]))[0][tfidf_sum.argsort()[::-1]][:top], \n",
    "                  np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('%s: %f' % (tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a problem, the number of features that we have created in ```doc_tfidf``` is huge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 516)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 500 features for merely 650 documents. In practice, this may lead to too much memory consumption (even with sparse matrix representation) if we have a large number of vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: Feature Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature hashing** reduces the dimension vocabulary space by hashing each vocabulary into a hash table with a fixed number of buckets. As compared to BoW, feature hashing has the following pros and cons:\n",
    "+ (+) no need to store vocabulary dictionary in memory anymore\n",
    "+ (-) no way to map token index back to token via ```inverse_transform()```\n",
    "+ (-) no IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**6)\n",
    "\n",
    "doc_hash = hashvec.transform(example_doc)\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can transform raw text to feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Creative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can go create your basic set of features for the text in competition. But **don't stop from here**. If you do aware the power of feature engineering, use your creativity to extract more features from the raw text. The more meaningful features you create, the more likely you will get a better score and win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are few examples for inspiration:\n",
    "+ [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "+ [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "+ [TextRank](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "+ [Latent Dirichlet Allocation](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "+ Similar word dictionary\n",
    "+ [Part-of-speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "\n",
    "There are lots of other directions you can explore, such as NLP features, length of lines, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint 2: Use Out-of-Core Learning If You Don't Have Enough Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of dataset in the competition is much larger than the lab. The dataset, after being represented as feature vectors, may become much larger, and you are unlikely to store all of them in memory. Next, we introduce another training technique called the **Out of Core Learning** to help you train a model using **data streaming**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of Out of Core Learning is similar to the **stochastic gradient descent**, which updates the model when seeing a minibatch, except that each minibatch is loaded from disk via a data stream. Since we only see a part of the dataset at a time, we can only use the ```HashingVectorizer``` to transform text into feature vectors because the ```HashingVectorizer``` does not require knowing the vocabulary space in advance.\n",
    "<br>\n",
    "<br>\n",
    "Let's create a stream to read a chunk of CSV file at a time using the Pandas I/O API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  This movie is well done on so many levels that...          1\n",
      "1  Wilson (Erica Gavin) is nabbed by the cops and...          1\n",
      "2  Canto 1: How Kriemhild Mourned Over Siegfried ...          1\n",
      "3  I bought Bloodsuckers on ebay a while ago. I w...          0\n",
      "4  I took part in a little mini production of thi...          1\n",
      "5  This is certainly one of my all time fav episo...          1\n",
      "6  This scary and rather gory adaptation of Steph...          1\n",
      "7  Mike Hawthorne(Gordon Currie)is witness to the...          0\n",
      "8  It looks to me as if the creators of \"The Clas...          0\n",
      "9  This comic book style film is funny, has nicel...          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "print(next(get_stream(path='imdb.csv', size=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Our stream works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For out-of core learning, we have to use models that can train and update the model's weight iteratively. Here, we use the SGDClassifier to train a LogisticRegressor using the stochastic gradient descent. We can partial update SGDClassifier by calling the ```partial_fit()``` method. Our workflow now becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stream documents directly from disk to get a mini-batch (chunk) of documents;\n",
    "2. Preprocess: clean words in the mini-batch of documents;\n",
    "3. Word2vec: use HashingVectorizer to extract features from text;\n",
    "4. Update ```SGDClassifier``` and go back to step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is a sanity check  :( ;P\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "print(preprocessor('<a href=\"example.com\">Hello, This :-( is a sanity check ;P!</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/25000] 0.883333\n",
      "[4000/25000] 0.910172\n",
      "[6000/25000] 0.909240\n",
      "[8000/25000] 0.911040\n",
      "[10000/25000] 0.936461\n",
      "[12000/25000] 0.908915\n",
      "[14000/25000] 0.936745\n",
      "[16000/25000] 0.939940\n",
      "[18000/25000] 0.943612\n",
      "[20000/25000] 0.928762\n",
      "[22000/25000] 0.925087\n",
      "[24000/25000] 0.943273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**20, \n",
    "                            preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', n_iter=100)\n",
    "\n",
    "batch_size = 1000\n",
    "stream = get_stream(path='imdb.csv', size=batch_size)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "train_auc, val_auc = [], []\n",
    "\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((25000+batch_size-1)/(batch_size*2))\n",
    "\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['review'], batch['sentiment']\n",
    "    if X_train is None:\n",
    "        break\n",
    "        \n",
    "    X_train = hashvec.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    train_auc.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "    \n",
    "    # validate\n",
    "    batch = next(stream)\n",
    "    X_val, y_val = batch['review'], batch['sentiment']\n",
    "    score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(X_val))[:,1])\n",
    "    val_auc.append(score)\n",
    "    print('[%d/%d] %f' % ((i+1)*(batch_size*2), 25000, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting SGDClassifier by an entire pass over training set, let's plot the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgmJXGKiIiIaFISp4iIiIgm/T9jVSRAeSf6FwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe129fb9f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\n",
    "plt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('#Batches')\n",
    "plt.ylabel('Auc')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./fig-out-of-core.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve looks great! The validation accuracy improves as more examples are seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training ```SGDClassifier``` may take long, you can save your trained classifier to disk periodically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.947\n"
     ]
    }
   ],
   "source": [
    "# import optimized pickle written in C for serializing and  de-serializing a Python object\n",
    "import _pickle as pkl\n",
    "\n",
    "# dump to disk\n",
    "pkl.dump(hashvec, open('hashvec.pkl', 'wb'))\n",
    "pkl.dump(clf, open('clf-sgd.pkl', 'wb'))\n",
    "\n",
    "# load from disk\n",
    "hashvec = pkl.load(open('hashvec.pkl', 'rb'))\n",
    "clf = pkl.load(open('clf-sgd.pkl', 'rb'))\n",
    "\n",
    "df_test = pd.read_csv('imdb.csv')\n",
    "print('test auc: %.3f' % roc_auc_score(df_test['sentiment'], \n",
    "                                       clf.predict_proba(hashvec.transform(df_test['review']))[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have the all the supporting knowledge for the competition. Happy coding and good luck!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
