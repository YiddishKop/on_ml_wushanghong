* Tensorflow
tf.InteractiveSession()
tf.Session()

* Train
tf.train.AdamOptimizer
tf.train.AdadeltaOptimizer
tf.train.GrandientDescentOptimizer
tf.train.RMSPropOptimizer
-------
tf.train.Example
tf.train.Features

* Summary
tf.summary.FileWriter
tf.summary.histogram
tf.summary.merge_all
tf.summary.scalar

* Loss
tf.losses.mean_square_error
tf.losses.softmax_cross_entropy

* Image
tf.image.random_flip_left_right
tf.image.random_brightness
tf.image.random_contrast
tf.image.per_image_standardization
tf.image.resize_image_with_crop_or_pad
tf.image.decode_image
tf.image.convert_image_dtype

* Contrib
*** tf.contrib.framework.get_or_create_global_step
    #+BEGIN_EXAMPLE
    tf.contrib.framework.get_or_create_global_step(graph=None)
    #+END_EXAMPLE
    Returns and create (if necessary) the global step tensor. (deprecated)

THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating: Please switch to ~tf.train.get_or_create_global_step~
*** tf.contrib.layers.flatten
    #+BEGIN_EXAMPLE
    tf.contrib.layers.flatten(
    inputs,
    outputs_collections=None,
    scope=None
)
    #+END_EXAMPLE

Flattens the input while maintaining the batch_size.

Assumes that the first dimension represents the batch.

Args:
- inputs: A tensor of size [batch_size, ...].
- outputs_collections: Collection to add the outputs.
- scope: Optional scope for name_scope.

Returns:
A flattened tensor with shape [batch_size, k].
*** tf.contrib.training.HParams
    Class to hold a set of hyperparameters as name-value pairs.

A HParams object holds hyperparameters used to build and train a model, such as
the number of hidden units in a neural net layer or the learning rate to use
when training.

You first create a HParams object by specifying the names and values of the
hyperparameters.

To make them easily accessible the parameter names are added as direct
attributes of the class. A typical usage is as follows:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Create a HParams object specifying names and values of the model
# hyperparameters:
hparams = HParams(learning_rate=0.1, num_hidden_units=100)

# The hyperparameter are available as attributes of the HParams object:
hparams.learning_rate ==> 0.1
hparams.num_hidden_units ==> 100
#+END_SRC
Hyperparameters have type, which is inferred from the type of their value passed
at construction type. The currently supported types are: integer, float,
boolean, string, and list of integer, float, boolean, or string.

You can override hyperparameter values by calling the parse() method, passing a
string of comma separated name=value pairs. This is intended to make it possible
to override any hyperparameter values from a single command-line flag to which
the user passes 'hyper-param=value' pairs. It avoids having to define one flag
for each hyperparameter.

The syntax expected for each value depends on the type of the parameter. See
parse() for a description of the syntax.

Example:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Define a command line flag to pass name=value pairs.
# For example using argparse:
import argparse
parser = argparse.ArgumentParser(description='Train my model.')
parser.add_argument('--hparams', type=str,
                    help='Comma separated list of "name=value" pairs.')
args = parser.parse_args()
...
def my_program():
  # Create a HParams object specifying the names and values of the
  # model hyperparameters:
  hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,
                       activations=['relu', 'tanh'])

  # Override hyperparameters values by parsing the command line
  hparams.parse(args.hparams)

  # If the user passed `--hparams=learning_rate=0.3` on the command line
  # then 'hparams' has the following attributes:
  hparams.learning_rate ==> 0.3
  hparams.num_hidden_units ==> 100
  hparams.activations ==> ['relu', 'tanh']

  # If the hyperparameters are in json format use parse_json:
  hparams.parse_json('{"learning_rate": 0.3, "activations": "relu"}')

#+END_SRC
*** tf.contrib.rnn.BasicRNNCell
*** tf.contrib.rnn.LSTMCell
*** tf.contrib.metrics.accuracy
*** tf.contrib.legacy_seq2seq.embedding_attention_seq2seq
    #+BEGIN_EXAMPLE
    tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(
    encoder_inputs,
    decoder_inputs,
    cell,
    num_encoder_symbols,
    num_decoder_symbols,
    embedding_size,
    num_heads=1,
    output_projection=None,
    feed_previous=False,
    dtype=None,
    scope=None,
    initial_state_attention=False
)
    #+END_EXAMPLE
Embedding sequence-to-sequence model with attention.

This model

- first embeds ~encoder_inputs~ by a newly created embedding (of shape
  ~[num_encoder_symbols x input_size]~). Then it runs an RNN to encode embedded
  ~encoder_inputs~ into a ~state vector~ (这个 state vector 就是 autoencoder 衔
  接 decoder 和 encoder 的 code, 是一个向量). It keeps the outputs of this RNN
  at every step to use for attention later.

- next, it embeds ~decoder_inputs~ by another newly created embedding (of shape
  ~[num_decoder_symbols x input_size]~). Then it runs attention decoder,
  initialized with the last encoder state, on embedded ~decoder_inputs~ and
  attending to encoder outputs.

Warning: when output_projection is None, the size of the attention vectors and
variables will be made proportional to num_decoder_symbols, can be large.

Args:
- encoder_inputs: A list of 1D int32 Tensors of shape [batch_size].
- decoder_inputs: A list of 1D int32 Tensors of shape [batch_size].
- cell: tf.nn.rnn_cell.RNNCell defining the cell function and size.
- num_encoder_symbols: Integer; number of symbols on the encoder side.
- num_decoder_symbols: Integer; number of symbols on the decoder side.
- embedding_size: Integer, the length of the embedding vector for each symbol.
- num_heads: Number of attention heads that read from attention_states.
- output_projection: None or a pair (W, B) of output projection weights and
  biases; W has shape [output_size x num_decoder_symbols] and B has shape
  [num_decoder_symbols]; if provided and feed_previous=True, each fed previous
  output will first be multiplied by W and added B.
- feed_previous: Boolean or scalar Boolean Tensor; if True, only the first of
  decoder_inputs will be used (the "GO" symbol), and all other decoder inputs
  will be taken from previous outputs (as in embedding_rnn_decoder). If False,
  decoder_inputs are used as given (the standard decoder case).
- dtype: The dtype of the initial RNN state (default: tf.float32).
- scope: VariableScope for the created subgraph; defaults to "embedding_attention_seq2seq".
- initial_state_attention: If False (default), initial attentions are zero. If
  True, initialize the attentions from the initial state and attention states.


Returns:

A tuple of the form (outputs, state), where: * outputs: A list of the same
length as decoder_inputs of 2D Tensors with shape [batch_size x
num_decoder_symbols] containing the generated outputs. * state: The state of
each decoder cell at the final time-step. It is a 2D Tensor of shape [batch_size
x cell.state_size].

*** tf.contrib.legacy_seq2seq.sequence_loss_by_example
    #+BEGIN_EXAMPLE
tf.contrib.legacy_seq2seq.sequence_loss_by_example(
    logits,
    targets,
    weights,
    average_across_timesteps=True,
    softmax_loss_function=None,
    name=None
)

    #+END_EXAMPLE
Defined in tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py.

Weighted cross-entropy loss for a sequence of logits (per example).

Args:

- logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].
- targets: List of 1D batch-sized int32 Tensors of the same length as logits.
- weights: List of 1D batch-sized float-Tensors of the same length as logits.
- average_across_timesteps: If set, divide the returned cost by the total label weight.
- softmax_loss_function: Function (labels, logits) -> loss-batch to be used
  instead of the standard softmax (the default if this is None). Note that to
  avoid confusion, it is required for the function to accept named arguments.
- name: Optional name for this operation, default: "sequence_loss_by_example".


Returns: 1D batch-sized float Tensor: The log-perplexity for each sequence.
* NN
*** ---- active fn ----
*** tf.nn.relu
*** tf.nn.sigmoid
*** ---- loss fn ----
*** tf.nn.l2_loss
*** tf.nn.l2_normalization
*** tf.nn.sigmoid_cross_entropy_with_logits
*** tf.nn.sparse_softmax_cross_entropy_with_logits
*** ---- training tech ----
*** tf.nn.dropout
*** tf.nn.softmax
*** tf.nn.batch_normalization
*** ---- CNN ----
*** tf.nn.conv2d
*** tf.nn.avg_pool
*** tf.nn.max_pool
*** ---- RNN ----
*** tf.nn.dynamic_rnn
*** tf.nn.rnn_cell.BasicLSTMCell
*** tf.nn.rnn_cell.DropoutWrapper
*** ---- k menas ----
*** tf.nn.top_k
*** tf.nn.in_top_k
*** ---- other ----
*** tf.nn.lrn
*** tf.nn.embedding_lookup
*** tf.nn.bias_add
    跟 add 差不多, 只不过是通过 broadcasting 的方式实现对每个 tensor 的元素都加
    上同一个数值: bias
    #+BEGIN_EXAMPLE
tf.nn.bias_add(
    value,
    bias,
    data_format=None,
    name=None
)
    #+END_EXAMPLE

Adds bias to value.

This is (mostly) a special case of tf.add where bias is restricted to 1-D.
Broadcasting is supported, so value may have any number of dimensions. Unlike
tf.add, the type of bias is allowed to differ from value in the case where both
types are quantized.

Args:
- value: A Tensor with type float, double, int64, int32, uint8, int16, int8, complex64, or complex128.
- bias: A 1-D Tensor with size matching the last dimension of value. Must be the same type as value unless value is a quantized type, in which case a different quantized type may be used.
- data_format: A string. 'NHWC' and 'NCHW' are supported.
- name: A name for the operation (optional).

Returns:
A Tensor with the same type as value.




* Layers
tf.layers.dense
tf.layers.conv2d
tf.layers.max_pooling2d

* Graph related
tf.Graph
tf.GraphDef

* Variable related
*** tf.Variable     ===> name_scope 会在其前面加上前缀作为最终变量名


See the Variables How To for a high level overview.

A variable maintains state in the graph across calls to run(). You add a
variable to the graph by constructing an instance of the class Variable.

The Variable() constructor requires an initial value for the variable, which can
be a Tensor of any type and shape. *The initial value defines the type and shape
of the variable*. *After construction, the type and shape of the variable are
fixed*. The value can be changed using one of the assign methods.

If you want to change the shape of a variable later you have to use an assign Op
with validate_shape=False.

Just like any Tensor,

*variables created with Variable()* can be

*used as inputs for other Ops in the graph*.

Additionally, all the operators overloaded for the Tensor class are carried over
to variables, so you can also add nodes to the graph by just doing arithmetic on
variables.


#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
  import tensorflow as tf

  # Create a variable.
  w = tf.Variable(<initial-value>, name=<optional-name>)

  # Use the variable in the graph like any Tensor.
  y = tf.matmul(w, ...another variable or tensor...)

  # The overloaded operators are available too.
  z = tf.sigmoid(w + y)

  # Assign a new value to the variable with `assign()` or a related method.
  w.assign(w + 1.0)
  w.assign_add(1.0)

#+END_SRC

When you launch the graph,

*variables have to be explicitly initialized before you can run Ops that use
their value*.

You can initialize a variable by running its

- *initializer op*
- *restoring the variable from a save file*
- *simply running an assign Op that assigns a value*

to the variable. In fact, the variable initializer op is just an assign Op that
assigns the variable's initial value to the variable itself.

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Launch the graph in a session.
with tf.Session() as sess:
    # Run the variable initializer.
    sess.run(w.initializer)
    # ...you now can run ops that use the value of 'w'...

#+END_SRC

The most common initialization pattern is to use the convenience function
~global_variables_initializer()~ to add an Op to the graph that initializes all
the variables. You then run that Op after launching the graph.

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Add an Op to initialize global variables.
init_op = tf.global_variables_initializer()

# Launch the graph in a session.
with tf.Session() as sess:
    # Run the Op that initializes global variables.
    sess.run(init_op)
    # ...you can now run any Op that uses variable values...

#+END_SRC

If you need to create a

*variable with an initial value dependent on another variable* ,

use the other variable's ~initialized_value()~. This ensures that variables are
initialized in the right order.

All variables are automatically collected in the graph where they are created.
By default, the constructor *adds the new variable to the graph collection*
~GraphKeys.GLOBAL_VARIABLES~. The convenience function ~global_variables()~
returns the contents of that collection.

When building a machine learning model it is often convenient to distinguish
between

- *variables holding the trainable model parameters*

and

- *other variables such as a global step variable used to count training steps*


To make this easier, the *variable constructor* supports a ~trainable=<bool>~
parameter. If True, the new variable is also added to the graph collection
~GraphKeys.TRAINABLE_VARIABLES~. The convenience function
~trainable_variables()~ returns the contents of this collection.

*The various Optimizer classes use this collection as the default list of
variables to optimize*.

WARNING: ~tf.Variable~ objects have a non-intuitive memory model. A Variable is
represented internally as a *mutable Tensor* which can non-deterministically alias
other Tensors in a graph. The set of operations which consume a Variable and can
lead to aliasing is undetermined and can change across TensorFlow versions.

#+BEGIN_QUOTE
Avoid writing code which relies on the value of a Variable either changing or
not changing as other operations happen.
#+END_QUOTE

For example, using Variable objects or simple functions thereof as predicates
in a ~tf.cond~ is dangerous and error-prone:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
v = tf.Variable(True)
tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.
#+END_SRC

Here replacing tf.Variable with tf.contrib.eager.Variable will fix any
nondeterminism issues.

To use the replacement for variables which does not have these issues:

- Replace ~tf.Variable~ with ~tf.contrib.eager.Variable~;
- Call ~tf.get_variable_scope().set_use_resource(True)~ inside a
  ~tf.variable_scope~ before the ~tf.get_variable()~ call.

*** Eager Compatibility
tf.Variable is not compatible with eager execution. Use tf.contrib.eager.Variable instead which is compatible with both eager execution and graph construction. See the TensorFlow Eager Execution guide for details on how variables work in eager execution.

*** tf.Variable.assign
*** tf.get_variable ===> name_scope 不会在其前面加上前缀作为最终变量名
*** tf.trainable_variables

* Math
** Arithmetic Operators
tf.add
tf.multiply

** Basic Math Functions
tf.pow
tf.log
tf.exp
tf.square
tf.round
tf.abs
tf.sqrt
tf.add_n ===> accept a list of Tensor, then do adding element-wise

** Matrix Math Functions
*** tf.transpose
*** tf.matmul
    #+BEGIN_EXAMPLE
tf.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)
    #+END_EXAMPLE

Args:
- a: Tensor of type float16, float32, float64, int32, complex64, complex128 and rank > 1.
- b: Tensor with same type and rank as a.
- transpose_a: If True, a is transposed before multiplication.
- transpose_b: If True, b is transposed before multiplication.
- adjoint_a: If True, a is conjugated and transposed before multiplication.
- adjoint_b: If True, b is conjugated and transposed before multiplication.
- a_is_sparse: If True, a is treated as a sparse matrix.
- b_is_sparse: If True, b is treated as a sparse matrix.
- name: Name for the operation (optional).

Returns:

A Tensor of the same type as a and b where each inner-most matrix is the product
of the corresponding matrices in a and b, e.g. if all transpose or adjoint
attributes are False:

output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j]), for all indices i, j.

Note:

This is matrix product, not element-wise product.

Raises:

ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.


Multiplies matrix a by matrix b, producing a * b.

The inputs must, following any transpositions, be tensors of rank >= 2 where the
inner 2 dimensions specify valid matrix multiplication arguments, and any
further outer dimensions match.

Both matrices must be of the same type. The supported types are: float16,
float32, float64, int32, complex64, complex128.

Either matrix can be transposed or adjointed (conjugated and transposed) on the
fly by setting one of the corresponding flag to True. These are False by
default.

If one or both of the matrices contain a lot of zeros, a more efficient
multiplication algorithm can be used by setting the corresponding a_is_sparse or
b_is_sparse flag to True. These are False by default. This optimization is only
available for plain matrices (rank-2 tensors) with datatypes bfloat16 or
float32.

For example:
#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python >= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])

#+END_SRC

** Tensor Match Function

** Complex Number Functions
** Reduction
tf.reduce_max
tf.reduce_mean

** Scan

** Segmentation

** Sequence Comparison and Indexing
   #+BEGIN_QUOTE
   TensorFlow provides several operations that you can use to add sequence
   comparison and index extraction to your graph. You can use these operations
   to determine sequence differences and determine the indexes of specific
   values in a tensor.
   #+END_QUOTE

*** tf.argmax
#+BEGIN_EXAMPLE
    tf.argmax(
    input,
    axis=None,
    name=None,
    dimension=None,
    output_type=tf.int64
)

Args:
- input: A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.
- axis: A Tensor. Must be one of the following types: int32, int64. int32 or int64, must be in the range [-rank(input), rank(input)). Describes which axis of the input Tensor to reduce across. For vectors, use axis = 0.
- output_type: An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.
- name: A name for the operation (optional).

Returns:
A Tensor of type output_type.
#+END_EXAMPLE

Returns the index with the largest value across axes of a tensor. (deprecated
arguments)

SOME ARGUMENTS ARE DEPRECATED. They will be removed in a future version.
Instructions for updating: Use the axis argument instead

Note that in case of ties the identity of the return value is not guaranteed.

*** tf.argmin
*** tf.where
*** tf.unique
    这个函数名字有歧义, 其实他是一个给Tensor *去重* 的函数, 返回一个元组 ~(去重
    后的Tensor, 以及去重前的坐标)~
    #+BEGIN_EXAMPLE
tf.unique(
    x,
    out_idx=tf.int32,
    name=None
)

Args:
x: A Tensor. 1-D.
out_idx: An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.
name: A name for the operation (optional).

Returns:
A tuple of Tensor objects (y, idx).
   - y: A Tensor. Has the same type as x.
   - idx: A Tensor of type out_idx.
    #+END_EXAMPLE


Finds unique elements in a 1-D tensor.

This operation returns a tensor y containing all of the unique elements of x
sorted in the same order that they occur in x. This operation also returns a
tensor idx the same size as x that contains the index of each value of x in the
unique output y. In other words:

y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]

For example:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
#+END_SRC


* Constants, Sequences, and Random Values

** Constant Value Tensor
tf.constant

#+BEGIN_EXAMPLE
    tf.constant(value,
                dtype=None,
                shape=None,
                name='Const',
                verify_shape=False
    )
#+END_EXAMPLE

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Constant 1-D Tensor populated with value list.
tensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) => [1 2 3 4 5 6 7]

# Constant 2-D tensor populated with scalar value -1.
tensor = tf.constant(-1.0, shape=[2, 3]) => [[-1. -1. -1.]
                                             [-1. -1. -1.]]
#+END_SRC


*** tf.zeros_like
*** tf.ones_like
*** tf.zeros
*** tf.ones

** Sequences
tf.linespace
tf.range

** Random Tensors
*** intro
 #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
   # Create a tensor of shape [2, 3] consisting of random normal values, with mean
   # -1 and standard deviation 4.
   norm = tf.random_normal([2, 3], mean=-1, stddev=4)

   # Shuffle the first dimension of a tensor
   c = tf.constant([[1, 2], [3, 4], [5, 6]])
   shuff = tf.random_shuffle(c)

   # Each time we run these ops, different results are generated
   sess = tf.Session()
   print(sess.run(norm))
   print(sess.run(norm))

   # Set an op-level seed to generate repeatable sequences across sessions.
   norm = tf.random_normal([2, 3], seed=1234)
   sess = tf.Session()
   print(sess.run(norm))
   print(sess.run(norm))
   sess = tf.Session()
   print(sess.run(norm))
   print(sess.run(norm))

   #Another common use of random values is the initialization of variables. Also
   #see the Variables How To.

   # Use random uniform values in [0, 1) as the initializer for a variable of shape
   # [2, 3]. The default type is float32.
   var = tf.Variable(tf.random_uniform([2, 3]), name="var")
   init = tf.global_variables_initializer()

   sess = tf.Session()
   sess.run(init)
   print(sess.run(var))
 #+END_SRC

*** tf.truncated_normal
 #+BEGIN_EXAMPLE python
 tf.truncated_normal(
     shape,
     mean=0.0,
     stddev=1.0,
     dtype=tf.float32,
     seed=None,
     name=None
 )
 #+END_EXAMPLE

 #+BEGIN_QUOTE
 The generated values follow a normal distribution with specified mean and
 standard deviation, except that values whose magnitude is more than 2 standard
 deviations from the mean are dropped and re-picked.
 #+END_QUOTE

*** tf.random_crop
*** tf.random_normal
*** tf.random_uniform
*** tf.multinomial
*** tf.set_random_seed

* Control Flow

** control flow operations
tf.group
tf.no_op
tf.cond

** comparison operators
tf.equal
tf.where

* Dataset Input Pipeline
~tf.data.Dataset~ allows you to build complex input pipelines. See the Importing
Data for an in-depth explanation of how to use this API.

** Reader classes
   Classes that create a dataset from input files.
*** tf.data.TFRecordDataset
A Dataset comprising records from one or more TFRecord files.
** Creating new datasets
   Static methods in Dataset that create new datasets.
*** tf.contrib.data.Dataset.from_tensor_slices

** Iterating over datasets
   These functions make a tf.data.Iterator from a Dataset.

   tf.data.Dataset.make_initializable_iterator
   tf.data.Dataset.make_one_shot_iterator

   The ~Iterator~ class also contains static methods that create a
   ~tf.data.Iterator~ that can be used with multiple Dataset objects.
*** tf.contrib.data.Iterator.from_string_handle
    #+BEGIN_EXAMPLE
@staticmethod
from_string_handle(
    string_handle,
    output_types,
    output_shapes=None,
    output_classes=None
)
    #+END_EXAMPLE
Creates a new, uninitialized Iterator based on the given handle.

This method allows you to define a "feedable" iterator where you can choose
between concrete iterators by feeding a value in a ~tf.Session.run~ call. In
that case, ~string_handle~ would a ~tf.placeholder~, and you would feed it with
the value of ~tf.data.Iterator.string_handle~ in each step.

For example, if you had two iterators that marked the current position in a
training dataset and a test dataset, you could choose which to use in each step
as follows:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
train_iterator = tf.data.Dataset(...).make_one_shot_iterator()
train_iterator_handle = sess.run(train_iterator.string_handle())

test_iterator = tf.data.Dataset(...).make_one_shot_iterator()
test_iterator_handle = sess.run(test_iterator.string_handle())

handle = tf.placeholder(tf.string, shape=[])
iterator = tf.data.Iterator.from_string_handle(
    handle, train_iterator.output_types)

next_element = iterator.get_next()
loss = f(next_element)

train_loss = sess.run(loss, feed_dict={handle: train_iterator_handle})
test_loss = sess.run(loss, feed_dict={handle: test_iterator_handle})
#+END_SRC

Args:
- string_handle: A scalar ~tf.Tensor~ of type ~tf.string~ that evaluates to a
  handle produced by the ~Iterator.string_handle()~ method.
- output_types: A nested structure of ~tf.DType~ objects corresponding to each
  component of an element of this dataset.
- output_shapes: (Optional.) A nested structure of ~tf.TensorShape~ objects
  corresponding to each component of an element of this dataset. If omitted,
  each component will have an unconstrainted shape.
- output_classes: (Optional.) A nested structure of Python type objects
  corresponding to each component of an element of this iterator. If omitted,
  each component is assumed to be of type tf.Tensor.

Returns:
An Iterator.

* Data IO(python functions)
*** TFRecords file format
 A TFRecords file represents a sequence of (binary) strings. The format is not
 *random access*, so it is *suitable for streaming large amounts of data* but not
 suitable if fast sharding or other non-sequential access is desired.

*** TFRecords Format Details

 A *TFRecords file* contains a sequence of strings with CRC32C (32-bit CRC using
 the Castagnoli polynomial) hashes. Each record has the format

 #+BEGIN_QUOTE
 uint64 length
 uint32 masked_crc32_of_length
 byte   data[length]
 uint32 masked_crc32_of_data
 #+END_QUOTE

 and the records are concatenated together to produce the file. CRCs are
 described here, and the mask of a CRC is

 #+BEGIN_QUOTE
 masked_crc = ((crc >> 15) | (crc << 17)) + 0xa282ead8ul
 #+END_QUOTE

*** tf.python_io.TFRecordWriter
    A class to write records to a TFRecords file.

    This class implements __enter__ and __exit__, and can be used in with blocks
    like a normal file.
*** tf.python_io.tf_record_iterator
    #+BEGIN_EXAMPLE
    tf.python_io.tf_record_iterator(
    path,
    options=None
)
    #+END_EXAMPLE

An iterator that read the records from a TFRecords file.

Args:
 - path: The path to the TFRecords file.
 - options: (optional) A TFRecordOptions object.


 Yields:
 Strings.

* Inputs and Readers
** Placeholers
   TensorFlow provides a placeholder operation that must be fed with data on
   execution. For more info, see the section on Feeding data.

*** tf.placeholder
*** tf.placeholder_with_default


*** tf.sparse_placeholder
For feeding SparseTensors which are composite type, there is a convenience
function:

** Readers
*** tf.FixedLengthRecorderReader

** Converting
*** tf.decode_raw
    #+BEGIN_EXAMPLE
    tf.decode_raw(
    bytes,
    out_type,
    little_endian=True,
    name=None
)
    #+END_EXAMPLE

See the guides: Inputs and Readers > Converting, Reading data > QueueRunner,
Strings > Conversion

Reinterpret the bytes of a *string* as a *vector of numbers*.

** Example protocol buffer
   TensorFlow's recommended format for training examples is serialized ~Example~
   protocol buffers, described here. They contain Features, described here.
*** tf.FixedLenFeature
    Configuration for parsing a *variable-length* input feature.
*** tf.VarLenFeature
    Configuration for parsing a *fixed-length* input feature.

    To treat sparse input as dense, provide a ~default_value~; otherwise, the
    parse functions will fail on any examples missing this feature.
** Queues
   TensorFlow provides several implementations of 'Queues', which are structures
   within the TensorFlow computation graph to stage pipelines of tensors
   together. The following describe the basic Queue interface and some
   implementations. To see an example use, see Threading and Queues.

** Conditional Accumulators

** Dealing with the filesystem

*** tf.read_file
    #+BEGIN_EXAMPLE
tf.read_file(
    filename,
    name=None
)
    #+END_EXAMPLE
See the guide: Inputs and Readers > Dealing with the filesystem

Reads and outputs the entire contents of the input filename.

** Input pipeline
   TensorFlow functions for setting up an input-prefetching pipeline. Please see
   the reading data how-to for context.

   https://www.tensorflow.org/images/AnimatedFileQueues.gif

   tf.errors.OutOfRangeError 这个错误会被下面两套 api 丢出, 所以放在这里.
** Beginning of an input pipeline
   The "producer" functions add a queue to the graph and a corresponding
   QueueRunner for running the subgraph that fills that queue.
*** tf.train.string_input_producer
    #+BEGIN_EXAMPLE
    tf.train.string_input_producer(
    string_tensor,
    num_epochs=None,
    shuffle=True,
    seed=None,
    capacity=32,
    shared_name=None,
    name=None,
    cancel_op=None
)
    #+END_EXAMPLE

    #+BEGIN_EXAMPLE
    files(dataset)
    +----.
    |     \
    |  +----.                 queue
    |  |     \            +-----+-----+-----+-----+  input
    +--|  +----.   -----> |     |     |     |     | =======> (graph) ML Model
       |  |     \         +-----+-----+-----+-----+
       +--|     |
          |     |
          +-----+
    #+END_EXAMPLE
Output strings (e.g. filenames) to a queue for an input pipeline.

Note: if ~num_epochs~ is not None, this function creates local counter epochs.
Use ~local_variables_initializer()~ to initialize local variables.

** Batching at the end of an input pipeline
   These functions add a queue to the graph to assemble a batch of examples,
   with possible shuffling. They also add a QueueRunner for running the subgraph
   that fills that queue.

Use ~tf.train.batch~ or ~tf.train.batch_join~ for batching examples that have
already been well shuffled. Use ~tf.train.shuffle_batch~ or
~tf.train.shuffle_batch_join~ for examples that would benefit from additional
shuffling.

Use ~tf.train.batch~ or ~tf.train.shuffle_batch~ if you want a single thread
producing examples to batch, or if you have a single subgraph producing examples
but you want to run it in N threads (where you increase N until it can keep the
queue full). Use ~tf.train.batch_join~ or ~tf.train.shuffle_batch_join~ if you
have N different subgraphs producing examples to batch and you want them run by
N threads. Use maybe_* to enqueue conditionally.

*** tf.train.shuffle_batch
    #+BEGIN_EXAMPLE
    tf.train.shuffle_batch(
    tensors,
    batch_size,
    capacity,
    min_after_dequeue,
    num_threads=1,
    seed=None,
    enqueue_many=False,
    shapes=None,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
    #+END_EXAMPLE

    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Creates batches of 32 images and 32 labels.
image_batch, label_batch = tf.train.shuffle_batch(
      [single_image, single_label],
      batch_size=32,
      num_threads=4,
      capacity=50000,
      min_after_dequeue=10000)
#+END_SRC
Args:
- tensors: The list or dictionary of tensors to enqueue.
- batch_size: The new batch size pulled from the queue.
- capacity: An integer. The maximum number of elements in the queue.
- min_after_dequeue: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.
- num_threads: The number of threads enqueuing tensor_list.
- seed: Seed for the random shuffling within the queue.
- enqueue_many: Whether each tensor in tensor_list is a single example.
- shapes: (Optional) The shapes for each example. Defaults to the inferred shapes for tensor_list.
- allow_smaller_final_batch: (Optional) Boolean. If True, allow the final batch to be smaller if there are insufficient items left in the queue.
- shared_name: (Optional) If set, this queue will be shared under the given name across multiple sessions.
- name: (Optional) A name for the operations.

Returns:
A list or dictionary of tensors with the types as tensors.

Creates batches by randomly shuffling tensors.

This function adds the following to the current Graph:

1. A shuffling queue into which tensors from tensors are enqueued.
2. A dequeue_many operation to create batches from the queue.
3. A QueueRunner to QUEUE_RUNNER collection, to enqueue the tensors from tensors.

If enqueue_many is False, tensors is assumed to represent a single example. An
input tensor with shape [x, y, z] will be output as a tensor with shape
[batch_size, x, y, z].

If enqueue_many is True, tensors is assumed to represent a batch of examples,
where the first dimension is indexed by example, and all members of tensors
should have the same size in the first dimension. If an input tensor has shape
[*, x, y, z], the output will have shape [batch_size, x, y, z].

The capacity argument controls the how long the prefetching is allowed to grow
the queues.

The returned operation is a dequeue operation and will throw
tf.errors.OutOfRangeError if the input queue is exhausted. If this operation is
feeding another input queue, its queue runner will catch this exception,
however, if this operation is used in your main thread you are responsible for
catching this yourself.

For example:

N.B.: You must ensure that either (i) the shapes argument is passed, or (ii) all
of the tensors in tensors must have fully-defined shapes. ValueError will be
raised if neither of these conditions holds.

If allow_smaller_final_batch is True, a smaller batch value than batch_size is
returned when the queue is closed and there are not enough elements to fill the
batch, otherwise the pending elements are discarded. In addition, all output
tensors' static shapes, as accessed via the shape property will have a first
Dimension value of None, and operations that depend on fixed batch_size would
fail.


Raises:
ValueError: If the shapes are not specified, and cannot be inferred from the elements of tensors.
*** tf.train.batch

    #+BEGIN_EXAMPLE
tf.train.batch(
    tensors,
    batch_size,
    num_threads=1,
    capacity=32,
    enqueue_many=False,
    shapes=None,
    dynamic_pad=False,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
    #+END_EXAMPLE


Creates batches of tensors in tensors.

The argument tensors can be a list or a dictionary of tensors. The value
returned by the function will be of the same type as tensors.

This function is implemented using a queue. A QueueRunner for the queue is added
to the current Graph's QUEUE_RUNNER collection.

If enqueue_many is False, tensors is assumed to represent a single example. An
input tensor with shape [x, y, z] will be output as a tensor with shape
[batch_size, x, y, z].

If enqueue_many is True, tensors is assumed to represent a batch of examples,
where the first dimension is indexed by example, and all members of tensors
should have the same size in the first dimension. If an input tensor has shape
[*, x, y, z], the output will have shape [batch_size, x, y, z]. The capacity
argument controls the how long the prefetching is allowed to grow the queues.

The returned operation is a dequeue operation and will throw
tf.errors.OutOfRangeError if the input queue is exhausted. If this operation is
feeding another input queue, its queue runner will catch this exception,
however, if this operation is used in your main thread you are responsible for
catching this yourself.

N.B.: If dynamic_pad is False, you must ensure that either
- (i) the shapes argument is passed, or
- (ii) all of the tensors in tensors must have fully-defined shapes. ValueError
  will be raised if neither of these conditions holds.

If dynamic_pad is True, it is sufficient that the rank of the tensors is known,
but individual dimensions may have shape None. In this case, for each enqueue
the dimensions with value None may have a variable length; upon dequeue, the
output tensors will be padded on the right to the maximum shape of the tensors
in the current minibatch. For numbers, this padding takes value 0. For strings,
this padding is the empty string. See PaddingFIFOQueue for more info.

If allow_smaller_final_batch is True, a smaller batch value than batch_size is
returned when the queue is closed and there are not enough elements to fill the
batch, otherwise the pending elements are discarded. In addition, all output
tensors' static shapes, as accessed via the shape property will have a first
Dimension value of None, and operations that depend on fixed batch_size would
fail.

* Training
** Optimizers

** Gradient Computation
TensorFlow provides functions to compute the derivatives for a given TensorFlow
computation graph, adding operations to the graph. The optimizer classes
automatically compute derivatives on your graph, but creators of new Optimizers
or expert users can call the lower-level functions below.

*** tf.gradients


** Gradient Clipping
TensorFlow provides several operations that you can use to add clipping
functions to your graph. You can use these functions to perform general data
clipping, but they're particularly useful for handling exploding or vanishing
gradients.

*** tf.clip_by_value
    #+BEGIN_EXAMPLE
    tf.clip_by_value(
    t,
    clip_value_min,
    clip_value_max,
    name=None
)

    #+END_EXAMPLE

Clips tensor values to a specified min and max.

#+BEGIN_QUOTE
Given a tensor t, this operation returns a tensor of the same type and shape as
t with its *values clipped to clip_value_min and clip_value_max*. Any values
less than ~clip_value_min~ are set to ~clip_value_min~. Any values greater than
~clip_value_max~ are set to ~clip_value_max~.
#+END_QUOTE

*** tf.clip_by_norm
    Clips tensor values to a maximum L2-norm.

Given a tensor t, and a maximum clip value clip_norm, this operation normalizes
t so that its *L2-norm* is less than or equal to clip_norm, along the dimensions
given in axes. Specifically, in the default case where all dimensions are used
for calculation, if the L2-norm of t is already less than or equal to clip_norm,
then t is not modified. If the L2-norm is greater than clip_norm, then this
operation returns a tensor of the same type and shape as t with its values set
to:

*t * clip_norm / l2_norm(t)*

In this case, the L2-norm of the output tensor is clip_norm.


#+DOWNLOADED: /tmp/screenshot.png @ 2018-07-25 17:11:29
[[file:other/screenshot_2018-07-25_17-11-29.png]]



** Decaying the learning rate
*** tf.train.exponential_decay
    #+BEGIN_EXAMPLE
    tf.train.exponential_decay(
    learning_rate,
    global_step,
    decay_steps,
    decay_rate,
    staircase=False,
    name=None
)
    #+END_EXAMPLE
When training a model, it is often recommended to *lower the learning rate as
the training progresses*. This function applies an *exponential decay* function
to a provided initial learning rate. It requires a ~global_step~ value to
compute the decayed learning rate. You can just pass a TensorFlow variable that
you increment at each training step.

#+BEGIN_EXAMPLE
decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)
#+END_EXAMPLE

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,
                                           100000, 0.96, staircase=True)
# Passing global_step to minimize() will increment it at each step.
learning_step = (
    tf.train.GradientDescentOptimizer(learning_rate)
    .minimize(...my loss..., global_step=global_step)
)
#+END_SRC


** Moving Average
Some training algorithms, such as *GradientDescent* and *Momentum* often benefit
from maintaining a moving average of variables during optimization. Using the
moving averages for evaluations often improve results significantly.

*** tf.train.ExponentialMovingAverage
Maintains moving averages of variables by employing an exponential decay.

When training a model, it is often beneficial to maintain moving averages of the
trained parameters. Evaluations that use averaged parameters sometimes produce
significantly better results than the final trained values.

The ~apply()~ method adds shadow copies of trained variables and add ops that
maintain a moving average of the trained variables in their shadow copies. It is
used when building the training model. The ops that maintain moving averages are
typically run after each training step. The ~average()~ and ~average_name()~
methods give access to the shadow variables and their names. They are useful
when building an evaluation model, or when restoring a model from a checkpoint
file. They help use the moving averages in place of the last trained values for
evaluations.

The moving averages are computed using exponential decay. You specify the decay
value when creating the ExponentialMovingAverage object. The shadow variables
are initialized with the same initial values as the trained variables. When you
run the ops to maintain the moving averages, each shadow variable is updated
with the formula:

#+BEGIN_EXAMPLE
shadow_variable -= (1 - decay) * (shadow_variable - variable)
#+END_EXAMPLE

This is mathematically equivalent to the classic formula below, but the use of
an assign_sub op (the "-=" in the formula) allows concurrent lockless updates to
the variables:

#+BEGIN_EXAMPLE
shadow_variable = decay * shadow_variable + (1 - decay) * variable
#+END_EXAMPLE

Reasonable values for decay are close to 1.0, typically in the multiple-nines
range: 0.999, 0.9999, etc.

Example usage when creating a training model:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...
...
# Create an op that applies the optimizer.  This is what we usually
# would use as a training op.
opt_op = opt.minimize(my_loss, [var0, var1])

# Create an ExponentialMovingAverage object
ema = tf.train.ExponentialMovingAverage(decay=0.9999)

with tf.control_dependencies([opt_op]):
    # Create the shadow variables, and add ops to maintain moving averages
    # of var0 and var1. This also creates an op that will update the moving
    # averages after each training step.  This is what we will use in place
    # of the usual training op.
    training_op = ema.apply([var0, var1])

...train the model by running training_op...
#+END_SRC

There are two ways to use the moving averages for evaluations:

1. Build a model that uses the shadow variables instead of the variables. For
   this, use the average() method which returns the shadow variable for a given
   variable.

2. Build a model normally but load the checkpoint files to evaluate by using the
   shadow variable names. For this use the average_name() method. See the
   tf.train.Saver for more information on restoring saved variables.


Example of restoring the shadow variable values:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Create a Saver that loads variables from their saved shadow values.
shadow_var0_name = ema.average_name(var0)
shadow_var1_name = ema.average_name(var1)
saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})
saver.restore(...checkpoint filename...)
# var0 and var1 now hold the moving average values

#+END_SRC
** Coordinator and QueueRunner
   See Threading and Queues for how to use threads and queues. For documentation
   on the Queue API, see Queues.

*** tf.train.Coordinator
    A coordinator for threads. This class implements a simple mechanism to
    coordinate the termination of a set of threads.

    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Create a coordinator.
coord = Coordinator()
# Start a number of threads, passing the coordinator to each of them.
...start thread 1...(coord, ...)
...start thread N...(coord, ...)
# Wait for all the threads to terminate.
coord.join(threads)
    #+END_SRC

    Any of the threads can call ~coord.request_stop()~ to ask for all the threads
    to stop. To cooperate with the requests, each thread must check for
    ~coord.should_stop()~ on a regular basis. ~coord.should_stop()~ returns True as
    soon as ~coord.request_stop()~ has been called.

A typical thread running with a coordinator will do something like:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
while not coord.should_stop():
  ...do some work...
#+END_SRC

*** tf.train.start_queue_runners
    #+BEGIN_EXAMPLE
    tf.train.start_queue_runners(
    sess=None,
    coord=None,
    daemon=True,
    start=True,
    collection=tf.GraphKeys.QUEUE_RUNNERS
)
    #+END_EXAMPLE

Starts all queue runners collected in the graph.

This is a companion method to ~add_queue_runner()~. It just starts threads for
all queue runners collected in the graph. It returns the list of all threads.

** Reading Summaries from Event Files
See Summaries and TensorBoard for an overview of summaries, event files, and
visualization in TensorBoard.

*** tf.train.summary_iterator
An iterator for reading Event protocol buffers from an event file.

You can use this function to read events written to an event file. It returns a
Python iterator that yields Event protocol buffers.

Example: Print the contents of an events file.

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
for e in tf.train.summary_iterator(path to events file):
    print(e)
#+END_SRC

Example: Print selected summary values.

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# This example supposes that the events file contains summaries with a
# summary value tag 'loss'.  These could have been added by calling
# `add_summary()`, passing the output of a scalar summary op created with
# with: `tf.summary.scalar('loss', loss_tensor)`.
for e in tf.train.summary_iterator(path to events file):
    for v in e.summary.value:
        if v.tag == 'loss':
            print(v.simple_value)
#+END_SRC
See the protocol buffer definitions of Event and Summary for more information
about their attributes.

* Variables
** Variables
tf.Variable     ===> name_scope 会在其前面加上前缀作为最终变量名

** varialbe helper functions
*** global_variables
    #+BEGIN_QUOTE
    Global variables are variables that are shared across machines in a
    *distributed environment*. The ~Variable()~ constructor or ~get_variable()~
    automatically adds new variables to the *graph collection*
    ~GraphKeys.GLOBAL_VARIABLES~. This convenience function returns the contents
    of that collection.
    #+END_QUOTE
*** loacal_variables
#+BEGIN_QUOTE
Local variables - *per process variables*, usually *not saved/restored to
checkpoint* and used for temporary or intermediate values. For example, they can
be used as counters for metrics computation or number of epochs this machine has
read data. The ~tf.contrib.framework.local_variable()~ function automatically
adds the new variable to ~GraphKeys.LOCAL_VARIABLES~. This convenience function
returns the contents of that collection.
#+END_QUOTE

*** tf.assign
    Update 'ref' by assigning 'value' to it.

#+BEGIN_QUOTE
This operation outputs a Tensor that holds the new value of 'ref' after the
value has been assigned. This makes it easier to chain operations that need to
use the reset value.
#+END_QUOTE

    #+BEGIN_EXAMPLE
    tf.assign(
    ref,
    value,
    validate_shape=None,
    use_locking=None,
    name=None
)
    #+END_EXAMPLE


*** tf.global_variables_initializer
    #+BEGIN_QUOTE
    Returns an Op that initializes global variables. This is just a shortcut for
    ~variables_initializer(global_variables())~
    #+END_QUOTE

** Saving and Restoring Variables
*** tf.train.Saver
    Saves and restores variables.

    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# Create a saver.
saver = tf.train.Saver(...variables...)
# Launch the graph and train, saving the model every 1,000 steps.
sess = tf.Session()
for step in xrange(1000000):
    sess.run(..training_op..)
    if step % 1000 == 0:
        # Append the step number to the checkpoint name:
        saver.save(sess, 'my-model', global_step=step)
    #+END_SRC
*** tf.train.latest_checkpoint
    #+BEGIN_EXAMPLE
    tf.train.latest_checkpoint(
    checkpoint_dir,
    latest_filename=None
)
    #+END_EXAMPLE
    Finds the filename of latest saved checkpoint file.
*** tf.train.get_checkpoint_state
    Returns CheckpointState proto from the "checkpoint" file if exist, None otherwise
** Sharing Variables
   TensorFlow provides several classes and operations that you can use to create
   variables contingent on certain conditions.

*** tf.get_variable
    Gets an existing variable with these parameters or create a new one.

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
def foo():
  with tf.variable_scope("foo", reuse=tf.AUTO_REUSE):
    v = tf.get_variable("v", [1])
  return v

v1 = foo()  # Creates v.
v2 = foo()  # Gets the same, existing v.
assert v1 == v2
#+END_SRC
*** tf.variable_scope
*** tf.constant_initializer
*** tf.random_normal_initializer
    Initializer that generates tensors with a normal distribution.
*** tf.truncated_normal_initializer

* Tensor Transformations
** Casting
   TensorFlow provides several operations that you can use to cast tensor data
   types in your graph

*** tf.cast ===> 对类型转换
** Shapes and Shaping
   TensorFlow provides several operations that you can use to determine the
   shape of a tensor and change the shape of a tensor.

*** tf.shape
*** tf.reshap
*** tf.squeeze
    #+BEGIN_EXAMPLE
    tf.squeeze(
         input,
         axis=None,
         name=None,
         squeeze_dims=None
    )
    #+END_EXAMPLE
    Removes dimensions of size 1 from the shape of a tensor.

    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
tf.shape(tf.squeeze(t))  # [2, 3]
Or, to remove specific size 1 dimensions:

# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]
    #+END_SRC

*** tf.expand_dims
    #+BEGIN_EXAMPLE
    tf.expand_dims(
    input,
    axis=None,
    name=None,
    dim=None
)
    #+END_EXAMPLE

Inserts a dimension of 1 into a tensor's shape. (deprecated arguments)

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
# 't' is a tensor of shape [2]
tf.shape(tf.expand_dims(t, 0))  # [1, 2]
tf.shape(tf.expand_dims(t, 1))  # [2, 1]
tf.shape(tf.expand_dims(t, -1)) # [2, 1]

# 't2' is a tensor of shape [2, 3, 5]
tf.shape(tf.expand_dims(t2, 0))  # [1, 2, 3, 5]
tf.shape(tf.expand_dims(t2, 2))  # [2, 3, 1, 5]
tf.shape(tf.expand_dims(t2, 3))  # [2, 3, 5, 1]
#+END_SRC

** Slicing and Joining
   TensorFlow provides several operations to slice or extract parts of a tensor,
   or join multiple tensors together.

*** tf.stack
    Stacks a list of rank-R tensors into one rank-(R+1) tensor.

    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
      x = tf.constant([1, 4])
      y = tf.constant([2, 5])
      z = tf.constant([3, 6])
      tf.stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)
      tf.stack([x, y, z], axis=1)  # [[1, 2, 3], [4, 5, 6]]
    #+END_SRC

    #+BEGIN_EXAMPLE
      # This is the opposite of unstack. The numpy equivalent is
      tf.stack([x, y, z]) = np.stack([x, y, z])
    #+END_EXAMPLE
*** tf.slice
    #+BEGIN_QUOTE
    Note that ~tf.Tensor.getitem~ is typically a more *pythonic* way to perform
    slices, as it allows you to write ~foo[3:7, :-2]~ instead of ~tf.slice(foo,
    [3, 0], [4, foo.get_shape()[1]-2])~.
    #+END_QUOTE
    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
      t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                       [[3, 3, 3], [4, 4, 4]],
                       [[5, 5, 5], [6, 6, 6]]])
      # t.shape() = (3,2,3)

      #   dimension-1 indices
      #   |
      #   |
      # [ 0 [[1, 1, 1], [2, 2, 2]],
      #
      #              /---- dimension-3 indices
      #       0  1  2
      #   1 [[3, 3, 3], [4, 4, 4]],
      #      ---------  ---------
      #          0          1   ---- dimension-2 indices
      #
      #   2 [[5, 5, 5], [6, 6, 6]]]

      tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]
      #            ^  ^  ^
      #            |  |  |
      #            |  |  |
      #            |  |  | 第三维度从 index=0 开始
      #            |  |
      #            |  |
      #            |  | 第二维度从 index=0 开始
      #            |
      #            |
      #            | 第一维度从 index=1 开始
      #

      tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],
                                         #   [4, 4, 4]]]
      tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],
                                         #  [[5, 5, 5]]]

    #+END_SRC
*** tf.strided_slice
    #+BEGIN_QUOTE
    strided 是跨距的意思

    Instead of calling this op directly most users will want to use the
    NumPy-style slicing syntax (e.g. ~tensor[..., 3:4:-1, tf.newaxis, 3]~),
    which is supported via ~tf.Tensor.getitem~ and ~tf.Variable.getitem~. The
    interface of this op is a low-level encoding of the slicing syntax.
    #+END_QUOTE

*** tf.split
    Splits a tensor into sub tensors.
    #+BEGIN_EXAMPLE
    tf.split(
    value,
    num_or_size_splits,
    axis=0, # along dimension 0
    num=None,
    name='split'
)
    #+END_EXAMPLE

    *along 谁, 就是谁不动, 拆另外一个*

    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
      # 'value' is a tensor with shape [5, 30]
      # Split 'value' into 3 tensors with sizes [4, 15, 11] along dimension 1
      split0, split1, split2 = tf.split(value, [4, 15, 11], 1)
      tf.shape(split0)  # [5, 4]
      tf.shape(split1)  # [5, 15]
      tf.shape(split2)  # [5, 11]
      # Split 'value' into 3 tensors along dimension 1
      split0, split1, split2 = tf.split(value, num_or_size_splits=3, axis=1)
      tf.shape(split0)  # [5, 10]
    #+END_SRC
*** tf.concat
    Concatenates tensors along one dimension.
    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
t1 = [[1, 2, 3], [4, 5, 6]]
t2 = [[7, 8, 9], [10, 11, 12]]
tf.concat([t1, t2], 0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]
tf.concat([t1, t2], 1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]

# tensor t3 with shape [2, 3]
# tensor t4 with shape [2, 3]
tf.shape(tf.concat([t3, t4], 0))  # [4, 3]
tf.shape(tf.concat([t3, t4], 1))  # [2, 6]
    #+END_SRC
*** tf.gather_nd
    #+BEGIN_EXAMPLE
    tf.gather_nd(
    params,
    indices,
    name=None
)
    #+END_EXAMPLE
    Gather slices from params into a Tensor with shape specified by indices

    #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
      # Simple indexing into a matrix:
      #
      #                         |第 1 维度 index=0    --
      #                         |                     |-- 'a'
      #                         |  |第 2 维度 index0  --
      #                         |  |
      #              整体看成坐标 (0, 0)
      #              ------
          indices = [[0, 0], [1, 1]]
          params = [['a', 'b'], ['c', 'd']]
          output = ['a', 'd']

      # Slice indexing into a matrix:
      #              |第 1 维度 index=0
      #              |选整个 index=0 对应的 list
      #              |
      #              |  |没有第 2 维度 index
      #              |  |
      #   整体看成坐标 (1)
          indices = [[1], [0]]
          params = [['a', 'b'], ['c', 'd']]
          output = [['c', 'd'], ['a', 'b']]
    #+END_SRC
*** tf.one_hot
    Returns a one-hot tensor.
#+BEGIN_EXAMPLE
tf.one_hot(
    indices,
    depth,
    on_value=None,
    off_value=None,
    axis=None,
    dtype=None,
    name=None
)
#+END_EXAMPLE

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
indices = [0, 1, 2]
depth = 3
tf.one_hot(indices, depth)  # output: [3 x 3]
# [[1., 0., 0.],
#  [0., 1., 0.],
#  [0., 0., 1.]]

indices = [0, 2, -1, 1]
depth = 3
tf.one_hot(indices, depth,
           on_value=5.0, off_value=0.0,
           axis=-1)  # output: [4 x 3]
# [[5.0, 0.0, 0.0],  # one_hot(0)
#  [0.0, 0.0, 5.0],  # one_hot(2)
#  [0.0, 0.0, 0.0],  # one_hot(-1)
#  [0.0, 5.0, 0.0]]  # one_hot(1)

indices = [[0, 2], [1, -1]]
depth = 3
tf.one_hot(indices, depth,
           on_value=1.0, off_value=0.0,
           axis=-1)  # output: [2 x 2 x 3]
# [[[1.0, 0.0, 0.0],   # one_hot(0)
#   [0.0, 0.0, 1.0]],  # one_hot(2)
#  [[0.0, 1.0, 0.0],   # one_hot(1)
#   [0.0, 0.0, 0.0]]]  # one_hot(-1)
#+END_SRC

*** tf.transpose

* Building Graphs
** Core graph data structures
tf.Graph
tf.Operation
tf.Tensor

** Tensor types
*** tf.DType
    ===> represent the type of elements in a Tensor
tf.float32
tf.int64
tf.unit8

*** tf.as_dtype()
    ===> converts numpy types and string type names to a DType object
** Utiliti functions
*** tf.device
*** tf.control_dependencies
    #+BEGIN_EXAMPLE
    control_dependencies(control_inputs)
    #+END_EXAMPLE
Returns a context manager that specifies control dependencies.

Use with the with keyword to specify that all operations constructed within the
context *should have control dependencies on control_inputs*. For example:

#+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
  with g.control_dependencies([a, b, c]):
    # `d` and `e` will only run after `a`, `b`, and `c` have executed.
    d = ...
    e = ...

  # Multiple calls to control_dependencies() can be nested, and in that case a
  # new Operation will have control dependencies on the union of control_inputs
  # from all active contexts.

  with g.control_dependencies([a, b]):
    # Ops constructed here run after `a` and `b`.
    with g.control_dependencies([c, d]):
      # Ops constructed here run after `a`, `b`, `c`, and `d`.

#+END_SRC


#+BEGIN_QUOTE
execution of ops created under this scope will trigger execution of the
dependencies
#+END_QUOTE

*** tf.name_scope
- tf.Variable     ===> name_scope 会在其前面加上前缀作为最终变量名
- tf.get_variable ===> name_scope 不会在其前面加上前缀作为最终变量名
- 且 name_scope 中无法对已有变量进行 reuse


- tf.Variable     ===> variable_scope 会在其前面加上前缀作为最终变量名
- tf.get_variable ===> variable_scope 会在其前面加上前缀作为最终变量名
- 且 variable_scope 可以对已有变量进行 reuse, 必须在 reuse variable 之前加上 scope.reuse_variables()
- 这种方式在 *RNN* 的实现中可能会用到
      #+BEGIN_SRC ipython :tangle yes :session :exports code :async t :results raw drawer
        from __future__ import print_function
        import tensorflow as tf
        tf.set_random_seed(1) # reproducible

        with tf.name_scope("a_name_scope") as scope:
            initializer = tf.constant_initializer(value=1)
            var1 = tf.get_variable(name='var1', shape=[1], dtype=tf.float32, initializer=initializer)
            var2 = tf.Variable(name='var2', initial_value=[2], dtype=tf.float32)
            var21 = tf.Variable(name='var2', initial_value=[2.1], dtype=tf.float32)
            var22 = tf.Variable(name='var2', initial_value=[2.2], dtype=tf.float32)

        with tf.variable_scope("a_variable_scope") as scope:
            initializer = tf.constant_initializer(value=3)
            var3 = tf.get_variable(name='var3', shape=[1], dtype=tf.float32, initializer=initializer)
            # ERROR
            # var3_resue = tf.get_variable(name='var1', shape=[1], dtype=tf.float32, initializer=initializer)

            # RIGHT
            scope.reuse_variables() # add this code before reuse declaration
            var3_resue = tf.get_variable(name='var1', shape=[1], dtype=tf.float32, initializer=initializer)

            var4 = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)
            var4_reuse = tf.Variable(name='var4', initial_value=[4], dtype=tf.float32)


        # 如果 graph 中已经有了相同的名字, 就会加上 _1 _2 _3 在名字之后
        with tf.Session() as sess:
            sess.run(tf.intialize_all_variables())
            print(var1.name)               # var1:0
            print(sess.run(var1))          # [1.]
            print(var2.name)               # a_name_scope/var2:0
            print(sess.run(var2))          # [2.]
            print(var21.name)              # a_name_scope/var2_1:0
            print(sess.run(var21))         # [2.0999999]
            print(var22.name)              # a_name_scope/var2_2:0
            print(sess.run(var22))         # [2.0000005]
            print(var3.name)               # a_variable_scope/var3:0
            print(sess.run(var3))          # [3.0000005]
            print(var3_reuse.name)         # a_variable_scope/var3:0
            print(sess.run(var3_resue))    # [3.0000005]
            print(var4.name)               # a_variable_scope/var4:0
            print(sess.run(var21))         # [4.0999999]
            print(var4_reuse.name)         # a_variable_scope/var4_1:0
            print(sess.run(var22))         # [4.0000005]
    #+END_SRC

*** tf.get_default_graph
*** tf.reset_default_graph

** Graph collections
*** tf.add_to_collection
#+BEGIN_EXAMPLE
add_to_collection(
    name,
    value
)
#+END_EXAMPLE

Stores value in the collection with the given name.

Note that collections are not sets, so it is possible to add a value to a
collection several times.



*** tf.get_collection
#+BEGIN_EXAMPLE
get_collection(
    name,
    scope=None
)
#+END_EXAMPLE
Returns a list of values in the collection with the given name.



* other
*** tf.Tensor.eval
*** tf.ConfigProto ==> A ProtocolMessage
*** tf.compat.as_str
    #+BEGIN_EXAMPLE
    tf.compat.as_str(
    bytes_or_text,
    encoding='utf-8'
)
    #+END_EXAMPLE

Returns the given argument as a unicode string.

*** tf.gfile.Exists
    #+BEGIN_EXAMPLE
tf.gfile.Exists(filename)
    #+END_EXAMPLE

    #+BEGIN_QUOTE
Determines whether a path exists or not. True if the path exists, whether its a
file or a directory. False if the path does not exist and there are no
filesystem errors.
    #+END_QUOTE
*** tf.parse_single_example
